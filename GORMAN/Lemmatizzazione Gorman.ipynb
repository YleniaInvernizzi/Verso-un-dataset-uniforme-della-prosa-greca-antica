{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_lis = os.listdir('output_gorman_con_relation')\n",
    "\n",
    "# Lista per salvare tutti i percorsi dei file CSV\n",
    "csv_file_paths = []\n",
    "\n",
    "# Directory principale che contiene le sottodirectory\n",
    "main_directory = 'output_gorman_con_relation'\n",
    "\n",
    "#per ogni cartella in output gorman devo ottenere la directory completa della singola cartella dell'autore. per farlo itero col ciclo for su tutte le cartelle presenti in gorman e concateno,\n",
    "#per ottenere la directory della cartella della singola opera, la main directory col nome della cratella.\n",
    "#per farlo uso la funzione di os che si chiama os.path join che fa una concatenazione di stringhe.\n",
    "for folder in dir_lis:\n",
    "  folder_path = os.path.join(main_directory, folder)\n",
    "  #check is folder_path is folder dir\n",
    "  if os.path.isdir(folder_path):  #ora voglio per ogni cartella una lista vuota poi da andare a riempire con i csv di ogni singola cartella.\n",
    "    singola_cartella = []\n",
    "    for file in os.listdir(folder_path): #per ogni file che sta nella mia cartella\n",
    "      if file.endswith('.csv'): #controllo per vedere se sono davvero tutti csv i file.\n",
    "        file_path = os.path.join(folder_path, file) #se il file è un csv la sua directory completa me la metti nella lista.\n",
    "        singola_cartella.append(file_path)\n",
    "\n",
    "  csv_file_paths.append(singola_cartella)\n",
    "csv_file_paths = sorted(csv_file_paths)\n",
    "# Lista per conservare i DataFrame filtrati\n",
    "#per ogni lista creo una lista di di dataframe. questi sono i csv filtrati con le info che mi servono.\n",
    "#ora lo trasformo in una funzione che prenda in input una lista di csv.\n",
    "def filter_dataframe(csv_file_paths):\n",
    "  dataframes_tot = [] #è una lista vuota in cui mettere i dataframe filtrati.\n",
    "# Leggi, filtra e aggiungi ogni DataFrame alla lista\n",
    "  for file_path in csv_file_paths:\n",
    "    # Leggi il CSV in un DataFrame\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filtra il DataFrame mantenendo solo le righe dove '@lemma' è una stringa\n",
    "      df_filtered = df[df['@lemma'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    # Aggiungi il DataFrame filtrato alla lista\n",
    "      dataframes_tot.append(df_filtered)\n",
    "\n",
    "# Visualizza le prime righe di ciascun DataFrame per controllare la struttura\n",
    "      data_info = {f\"File {i+1}\": df.head() for i, df in enumerate(dataframes_tot)}\n",
    "\n",
    "# Mostra la struttura dei dati\n",
    "      #for file_name, df_head in data_info.items():\n",
    "       # print(f\"\\n{file_name}:\")\n",
    "        #print(df_head)\n",
    "  return dataframes_tot\n",
    "# Function to extract the numeric part from the filename\n",
    "def extract_number(file_path):\n",
    "    # Use regex to find the numeric part of the filename\n",
    "    return int(re.search(r'(\\d+)\\.csv$', file_path).group(1))\n",
    "all_sorted_csv_filepaths = []\n",
    "for folder in csv_file_paths:\n",
    "  #print(folder)\n",
    "  sorted_files = sorted(folder, key=extract_number)\n",
    "  all_sorted_csv_filepaths.append(sorted_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [filter_dataframe(lis) for lis in all_sorted_csv_filepaths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir = os.listdir('output_gorman_con_relation')\n",
    "all_opera = {}\n",
    "for i,opera in enumerate(list_dir):\n",
    "    full_text = ''\n",
    "    for sent in dataframes[i]:\n",
    "        full_text += ' '.join(sent['@form'])\n",
    "        all_opera[opera] = full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_opera = {}\n",
    "for i,opera in enumerate(list_dir):\n",
    "    all_data = pd.concat(dataframes[i],ignore_index=True)\n",
    "   \n",
    "    #for sent in dataframes[i]:\n",
    "    full_text = ' '.join(all_data['@form'])\n",
    "    all_opera[opera] = full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opera_names = list(all_opera.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP ci indica il numero di frasi per chunk \n",
    "STEP = 18\n",
    "all_opera_split = {}\n",
    "for key in opera_names:\n",
    "    joint_text = []\n",
    "    text = all_opera[key]\n",
    "    sents = text.split('.')\n",
    "    for i in range(0, len(sents), STEP):\n",
    "        joint_text.append('.'.join(sents[i:i+STEP])+'.')\n",
    "    all_opera_split[key] = joint_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import os\\nimport shutil\\ndir = 'output_gorman'\\n\\nlis_dir = os.listdir(dir)\\nateneo = []\\nfor i in lis_dir:\\n    if i.split('-')[0]=='athen13' or i.split('-')=='athen12':\\n        ateneo.append(i)\\nateneo_to_remove = ['athen13-10-19-2019.xml',\\n 'athen13-20-29-2019.xml',\\n 'athen13-40-49-2019.xml',\\n 'athen13-70-79-2019.xml',\\n 'athen13-80-89-2019.xml',\\n 'athen13-90-95-2019.xml',\\n  'athen13-60-69-2019.xml',\\n 'athen13-50-59-2019.xml',\\n 'athen13-30-39-2019.xml',\\n 'athen13-1-9-2019.xml']\\ndest = 'output_gorman_ateneo_rimossi'\\nfor i in ateneo_to_remove:\\n    shutil.move(dir+'/'+i,dest +'/' + i )\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import shutil\n",
    "dir = 'output_gorman'\n",
    "\n",
    "lis_dir = os.listdir(dir)\n",
    "ateneo = []\n",
    "for i in lis_dir:\n",
    "    if i.split('-')[0]=='athen13' or i.split('-')=='athen12':\n",
    "        ateneo.append(i)\n",
    "ateneo_to_remove = ['athen13-10-19-2019.xml',\n",
    " 'athen13-20-29-2019.xml',\n",
    " 'athen13-40-49-2019.xml',\n",
    " 'athen13-70-79-2019.xml',\n",
    " 'athen13-80-89-2019.xml',\n",
    " 'athen13-90-95-2019.xml',\n",
    "  'athen13-60-69-2019.xml',\n",
    " 'athen13-50-59-2019.xml',\n",
    " 'athen13-30-39-2019.xml',\n",
    " 'athen13-1-9-2019.xml']\n",
    "dest = 'output_gorman_ateneo_rimossi'\n",
    "for i in ateneo_to_remove:\n",
    "    shutil.move(dir+'/'+i,dest +'/' + i )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import os\\nimport shutil\\ndir = 'output_gorman_con_relation'\\n\\nlis_dir = os.listdir(dir)\\nateneo = []\\nfor i in lis_dir:\\n    if i.split('-')[0]=='athen13' or i.split('-')=='athen12':\\n        ateneo.append(i)\\nateneo_to_remove = ['athen13-10-19-2019.xml',\\n 'athen13-20-29-2019.xml',\\n 'athen13-40-49-2019.xml',\\n 'athen13-70-79-2019.xml',\\n 'athen13-80-89-2019.xml',\\n 'athen13-90-95-2019.xml',\\n  'athen13-60-69-2019.xml',\\n 'athen13-50-59-2019.xml',\\n 'athen13-30-39-2019.xml',\\n 'athen13-1-9-2019.xml']\\ndest = 'output_gorman_ateneo_con_relation_rimossi'\\nfor i in ateneo_to_remove:\\n    shutil.move(dir+'/'+i,dest +'/' + i )\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import shutil\n",
    "dir = 'output_gorman_con_relation'\n",
    "\n",
    "lis_dir = os.listdir(dir)\n",
    "ateneo = []\n",
    "for i in lis_dir:\n",
    "    if i.split('-')[0]=='athen13' or i.split('-')=='athen12':\n",
    "        ateneo.append(i)\n",
    "ateneo_to_remove = ['athen13-10-19-2019.xml',\n",
    " 'athen13-20-29-2019.xml',\n",
    " 'athen13-40-49-2019.xml',\n",
    " 'athen13-70-79-2019.xml',\n",
    " 'athen13-80-89-2019.xml',\n",
    " 'athen13-90-95-2019.xml',\n",
    "  'athen13-60-69-2019.xml',\n",
    " 'athen13-50-59-2019.xml',\n",
    " 'athen13-30-39-2019.xml',\n",
    " 'athen13-1-9-2019.xml']\n",
    "dest = 'output_gorman_ateneo_con_relation_rimossi'\n",
    "for i in ateneo_to_remove:\n",
    "    shutil.move(dir+'/'+i,dest +'/' + i )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opera_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aeschines-1-1-50-bu1.xml',\n",
       " 'aeschines-1-101-150-bu1.xml',\n",
       " 'aeschines-1-151-196-bu1.xml',\n",
       " 'aeschines-1-51-100-bu1.xml',\n",
       " 'antiphon-1-bu2.xml',\n",
       " 'antiphon-2-bu2.xml',\n",
       " 'antiphon-5-bu2.xml',\n",
       " 'antiphon-6-bu2.xml',\n",
       " 'appian-bc-1-0-1-4-bu1.xml',\n",
       " 'appian-bc-1-11-14-bu1.xml',\n",
       " 'appian-bc-1-5-7-bu1.xml',\n",
       " 'appian-bc-1-8-10-bu1.xml',\n",
       " 'aristotle-politics-book-1-bu1.xml',\n",
       " 'aristotle-politics-book-2-bu2.xml',\n",
       " 'athen12-1-9-2019.xml',\n",
       " 'athen12-10-19-2019.xml',\n",
       " 'athen12-20-29-2019.xml',\n",
       " 'athen12-30-39-jan-15.xml',\n",
       " 'athen12-40-49-jan-15.xml',\n",
       " 'athen12-50-59-jan-15.xml',\n",
       " 'athen12-60-69-jan-15.xml',\n",
       " 'athen12-70-81-jan-15.xml',\n",
       " 'athen13-1-9-jan-15.xml',\n",
       " 'athen13-10-19-jan-15.xml',\n",
       " 'athen13-20-29-jan-15.xml',\n",
       " 'athen13-30-39-jan-15.xml',\n",
       " 'athen13-40-49-jan-15.xml',\n",
       " 'athen13-50-59-jan-15.xml',\n",
       " 'athen13-60-69-jan-15.xml',\n",
       " 'athen13-70-79-jan-15.xml',\n",
       " 'athen13-80-89-jan-15.xml',\n",
       " 'athen13-90-95-jan-15.xml',\n",
       " 'dem-59-neaira-2019.xml',\n",
       " 'demosthenes-1-bu1.xml',\n",
       " 'demosthenes-18-1-50-bu2.xml',\n",
       " 'demosthenes-18-101-150-bu2.xml',\n",
       " 'demosthenes-18-151-200-bu2.xml',\n",
       " 'demosthenes-18-201-275-bu1.xml',\n",
       " 'demosthenes-18-276-324-bu1.xml',\n",
       " 'demosthenes-18-51-100-bu1.xml',\n",
       " 'demosthenes-4-phil1-bu1.xml',\n",
       " 'demosthenes-46-tree.xml',\n",
       " 'demosthenes-47-tree.xml',\n",
       " 'demosthenes-49-tree.xml',\n",
       " 'demosthenes-50-tree.xml',\n",
       " 'demosthenes-52-tree.xml',\n",
       " 'demosthenes-53-tree.xml',\n",
       " 'diodsic-11-1-20-bu4.xml',\n",
       " 'diodsic-11-81-92-bu1.xml',\n",
       " 'diodsic11-21-40-bu2.xml',\n",
       " 'diodsic11-41-60-bu1.xml',\n",
       " 'diodsic11-61-80-bu1.xml',\n",
       " 'dion-hal-1-1-15-bu2.xml',\n",
       " 'dion-hal-1-16-30-bu1.xml',\n",
       " 'dion-hal-1-31-45-bu1.xml',\n",
       " 'dion-hal-1-46-60-bu1.xml',\n",
       " 'dion-hal-1-61-75-bu1.xml',\n",
       " 'dion-hal-1-76-90-bu1.xml',\n",
       " 'hdt-1-1-19-bu3-2019.xml',\n",
       " 'hdt-1-100-119-bu3-2019.xml',\n",
       " 'hdt-1-120-149-bu2-2019.xml',\n",
       " 'hdt-1-150-169-bu3-2019.xml',\n",
       " 'hdt-1-170-189-bu2-2019.xml',\n",
       " 'hdt-1-190-216-bu2-2019.xml',\n",
       " 'hdt-1-20-39-bu2-2019.xml',\n",
       " 'hdt-1-40-59-bu2-2019.xml',\n",
       " 'hdt-1-60-79-bu2-2019.xml',\n",
       " 'hdt-1-80-99-bu5-2019.xml',\n",
       " 'josephus-bj-1-1-2-bu1.xml',\n",
       " 'josephus-bj-1-11-15-bu1.xml',\n",
       " 'josephus-bj-1-16-20-bu1.xml',\n",
       " 'josephus-bj-1-21-25-bu1.xml',\n",
       " 'josephus-bj-1-3-5-bu2.xml',\n",
       " 'josephus-bj-1-6-10-bu1.xml',\n",
       " 'lysias-1-bu1.xml',\n",
       " 'lysias-12-bu1.xml',\n",
       " 'lysias-13-bu1.xml',\n",
       " 'lysias-14-bu1.xml',\n",
       " 'lysias-15.xml',\n",
       " 'lysias-19-bu1.xml',\n",
       " 'lysias-23-bu1.xml',\n",
       " 'plato-apology.xml',\n",
       " 'plut-alcib-1-17-bu1.xml',\n",
       " 'plut-alcib-18-39-bu1.xml',\n",
       " 'plut-fortuna-romanorum-bu1.xml',\n",
       " 'plutarch-alex-fort-aut-virt-bu2.xml',\n",
       " 'plutarch-lycurgus-1-15-bu4.xml',\n",
       " 'plutarch-lycurgus-16-31-bu2.xml',\n",
       " 'polybius-10-1-10-bu1.xml',\n",
       " 'polybius-10-11-20-bu1.xml',\n",
       " 'polybius-10-21-35-bu2.xml',\n",
       " 'polybius-10-36-49-bu1.xml',\n",
       " 'polybius-2-1-10-bu1.xml',\n",
       " 'polybius-2-11-20-bu1.xml',\n",
       " 'polybius-2-21-30-bu2.xml',\n",
       " 'polybius-2-31-40-bu2.xml',\n",
       " 'polybius-2-41-50-bu1.xml',\n",
       " 'polybius-2-51-60-bu1.xml',\n",
       " 'polybius-2-61-71-bu2.xml',\n",
       " 'polybius-21-1-10-bu1.xml',\n",
       " 'polybius-21-11-20-bu1.xml',\n",
       " 'polybius-21-21-30-bu1.xml',\n",
       " 'polybius-21-31-47-bu1.xml',\n",
       " 'polybius-6-16-30-bu1.xml',\n",
       " 'polybius-6-2-15-bu1.xml',\n",
       " 'polybius-6-31-45-bu1.xml',\n",
       " 'polybius-6-46-58-bu1.xml',\n",
       " 'polybius-9-1-20-bu1.xml',\n",
       " 'polybius-9-21-33-bu1.xml',\n",
       " 'polybius-9-34-45-bu1.xml',\n",
       " 'polybius1-1-9-2017.xml',\n",
       " 'polybius1-10-19-2017.xml',\n",
       " 'polybius1-20-29-2017.xml',\n",
       " 'polybius1-30-39-2017.xml',\n",
       " 'polybius1-40-49-2017.xml',\n",
       " 'polybius1-50-59-2017.xml',\n",
       " 'polybius1-60-69-2017.xml',\n",
       " 'polybius1-70-79-2017.xml',\n",
       " 'polybius1-80-88-2017.xml',\n",
       " 'ps-xen-ath-pol-bu2.xml',\n",
       " 'thuc-1-1-20-bu5.xml',\n",
       " 'thuc-1-101-120-bu2.xml',\n",
       " 'thuc-1-121-146-bu3.xml',\n",
       " 'thuc-1-21-40-bu4.xml',\n",
       " 'thuc-1-41-60-bu3.xml',\n",
       " 'thuc-1-61-80-bu3.xml',\n",
       " 'thuc-1-81-100-bu2.xml',\n",
       " 'thuc-3-1-20-bu1.xml',\n",
       " 'thuc-3-21-40-bu1.xml',\n",
       " 'xen-cyr-1-1-2-bu1.xml',\n",
       " 'xen-cyr-1-3-4-bu1.xml',\n",
       " 'xen-cyr-1-5-bu1.xml',\n",
       " 'xen-cyr-1-6-bu1.xml',\n",
       " 'xen-cyr-7-1-3-tree.xml',\n",
       " 'xen-cyr-7-4-5-tree.xml',\n",
       " 'xen-cyr-8-1-8-4-bu1.xml',\n",
       " 'xen-cyr-8-5-7-bu1.xml',\n",
       " 'xen-cyr-8-8-bu1.xml',\n",
       " 'xen-hell-1-1-4-bu2.xml',\n",
       " 'xen-hell-1-5-7-bu1.xml',\n",
       " 'xen-hell-2-bu1.xml',\n",
       " 'xen-hell-3-bu1.xml']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opera_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basandoti sulla tua conoscenza del greco data da Perseus Project e LSJ fai la lemmatizzazione del seguente testo, generando un dataframe csv che ha come prima colonna la form @form, e come colonne successive tutti i possibili lemmi di quella forma con i nomi @lemmaPerseus1, @lemmaPerseus2 etc... disponi ogni forma su una nuova riga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from openai import OpenAI \\nimport os\\n\\n## Set the API key and model name\\nMODEL=\"gpt-4o\"\\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"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\")\\n)'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from openai import OpenAI \n",
    "import os\n",
    "\n",
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"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\")\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opera_names[33]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''opera_names[33]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Il testo da analizzare\\ngreek_text = all_opera[opera_names[33]]\\n\\nprompt = f\"\"\"Basandoti sulla tua conoscenza del greco data da Perseus Project e LSJ fai la lemmatizzazione del seguente testo, generando un dataframe csv che ha come prima colonna la form @form, e come colonne successive tutti i possibili lemmi di quella forma con i nomi @lemmaPerseus1, @lemmaPerseus2 etc... disponi ogni forma su una nuova riga:\\n\\n{greek_text}\\n\"\"\"\\n\\ncompletion = client.chat.completions.create(\\n    model=MODEL,\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"Sei un assistente utile e competente in greco antico, con accesso virtuale a Perseus Project e LSJ.\"},\\n        {\"role\": \"user\", \"content\": prompt}\\n    ],\\n    temperature=0.0\\n)\\n\\n# Stampa la risposta del modello\\nprint(completion.choices[0].message.content)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Il testo da analizzare\n",
    "greek_text = all_opera[opera_names[33]]\n",
    "\n",
    "prompt = f\"\"\"Basandoti sulla tua conoscenza del greco data da Perseus Project e LSJ fai la lemmatizzazione del seguente testo, generando un dataframe csv che ha come prima colonna la form @form, e come colonne successive tutti i possibili lemmi di quella forma con i nomi @lemmaPerseus1, @lemmaPerseus2 etc... disponi ogni forma su una nuova riga:\n",
    "\n",
    "{greek_text}\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Sei un assistente utile e competente in greco antico, con accesso virtuale a Perseus Project e LSJ.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Stampa la risposta del modello\n",
    "print(completion.choices[0].message.content)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Il testo da analizzare\\ngreek_text = all_opera[opera_names[0]]\\n\\nprompt = f\"\"\"Basandoti sulla tua conoscenza del greco data da Perseus Project e LSJ fai il pos tagging in formato AGDT (ANCIENT GREEK DEPENDENCY TREEBANK, per es. per l\\'avverbio è così d--------) \\ndelle particelle \\ndel seguente testo, generando un dataframe csv che ha come prima colonna la form @form,\\n e come SECONDA COLONNA il pos tagging delle particelle e un trattino delle non particelle. \\n Per particelle greche intendo ad esempio ἢ,ἤπερ,\\'ἀλλʼ\\', \\'ἀλλά\\', \\'ἀλλὰ\\', \\'ἀλλ’\\',\"οὔτε\", \"μήτε\",\"οὐτ\\'\", \"μὴτ\\'\",\"οὔθʼ\", \"μὴθʼ\",\\'μήθ’\\',\\'μήτ’\\',\\'οὔτ’\\',\\'οὔτʼ\\',\\'οὔθ’\\',\"εἴτε\",\"-τ\\'\", \"τ\\'\",\\n   \"τε\", \"-τ\\' \", \"τ\\' \", \"τε \", \\'τέ\\',\\'-θ’\\',\\'θ’\\',\\'-τ’\\',\\'τ’\\',\\'-τε\\',\\'τʼ\\',\\'μέν\\', \\'-δέ\\', \\'δέ\\',\\'μὲν\\', \\'δὲ\\', \\'-δʼ\\',\\'-δ’\\',\\'δʼ,\\',\\'δ’\\',\\'-δὲ\\',\\'δʼ\\',δή etc. \\n   Cerca di capire dal contesto se si tratta di avverbi o di congiunzioni coordinanti o di congiunzioni subordinanti etc. e fai il pos tagging di conseguenza... \\n   disponi ogni forma su una nuova riga. Fai tutto il testo per intero:\\n\\n{greek_text}\\n\"\"\"\\n\\ncompletion = client.chat.completions.create(\\n    model=MODEL,\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"Sei un assistente utile e competente in greco antico, con accesso virtuale a Perseus Project e LSJ.\"},\\n        {\"role\": \"user\", \"content\": prompt}\\n    ],\\n    temperature=0.0, max_tokens = 16384\\n)\\n\\n# Stampa la risposta del modello\\nprint(completion.choices[0].message.content)'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Il testo da analizzare\n",
    "greek_text = all_opera[opera_names[0]]\n",
    "\n",
    "prompt = f\"\"\"Basandoti sulla tua conoscenza del greco data da Perseus Project e LSJ fai il pos tagging in formato AGDT (ANCIENT GREEK DEPENDENCY TREEBANK, per es. per l'avverbio è così d--------) \n",
    "delle particelle \n",
    "del seguente testo, generando un dataframe csv che ha come prima colonna la form @form,\n",
    " e come SECONDA COLONNA il pos tagging delle particelle e un trattino delle non particelle. \n",
    " Per particelle greche intendo ad esempio ἢ,ἤπερ,'ἀλλʼ', 'ἀλλά', 'ἀλλὰ', 'ἀλλ’',\"οὔτε\", \"μήτε\",\"οὐτ'\", \"μὴτ'\",\"οὔθʼ\", \"μὴθʼ\",'μήθ’','μήτ’','οὔτ’','οὔτʼ','οὔθ’',\"εἴτε\",\"-τ'\", \"τ'\",\n",
    "   \"τε\", \"-τ' \", \"τ' \", \"τε \", 'τέ','-θ’','θ’','-τ’','τ’','-τε','τʼ','μέν', '-δέ', 'δέ','μὲν', 'δὲ', '-δʼ','-δ’','δʼ,','δ’','-δὲ','δʼ',δή etc. \n",
    "   Cerca di capire dal contesto se si tratta di avverbi o di congiunzioni coordinanti o di congiunzioni subordinanti etc. e fai il pos tagging di conseguenza... \n",
    "   disponi ogni forma su una nuova riga. Fai tutto il testo per intero:\n",
    "\n",
    "{greek_text}\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Sei un assistente utile e competente in greco antico, con accesso virtuale a Perseus Project e LSJ.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.0, max_tokens = 16384\n",
    ")\n",
    "\n",
    "# Stampa la risposta del modello\n",
    "print(completion.choices[0].message.content)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'polybius1-70-79-2017.xml'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ὁ δὲ Γέσκων ἑώρα μὲν τὴν ὅλην ἀκαταστασίαν καὶ ταραχήν , περὶ πλείστου δὲ ποιούμενος τὸ τῇ πατρίδι συμφέρον καὶ θεωρῶν ὅτι τούτων ἀποθηριωθέντων κινδυνεύουσι προφανῶς οἱ Καρχηδόνιοι τοῖς ὅλοις πράγμασι , παρεβάλλετο καὶ προσεκαρτέρει , ποτὲ μὲν τοὺς προεστῶτας αὐτῶν εἰς τὰς χεῖρας λαμβάνων , ποτὲ δὲ κατὰ γένη συναθροίζων καὶ παρακαλῶν . οὐ μὴν ἀλλὰ τῶν Λιβύων οὐδέπω κεκομισμένων τὰς σιταρχίας , οἰομένων δὲ δεῖν ἀποδεδόσθαι σφίσι καὶ προσιόντων θρασέως , βουλόμενος ὁ Γέσκων ἐπιπλῆξαι τὴν προπέτειαν αὐτῶν , Μάθω τὸν στρατηγὸν ἀπαιτεῖν ἐκέλευεν . οἱ δ᾽ ἐπὶ τοσοῦτον διωργίσθησαν ὥστ᾽ οὐδὲ τὸν τυχόντα χρόνον ἀναστροφὴν δόντες ὥρμησαν τὸ μὲν πρῶτον ἐπὶ τὸ διαρπάζειν τὰ πρόχειρα τῶν χρημάτων , μετὰ δὲ ταῦτα συλλαμβάνειν τόν τε Γέσκωνα καὶ τοὺς μετ᾽ αὐτοῦ Καρχηδονίους . οἱ δὲ περὶ τὸν Μάθω καὶ τὸν Σπένδιον ὑπολαμβάνοντες τάχιστ᾽ ἂν οὕτως ἐκκαυθῆναι τὸν πόλεμον , εἰ παράνομόν τι πράξειαν καὶ παράσπονδον , συνήργουν ταῖς τῶν ὄχλων ἀπονοίαις καὶ τὴν μὲν ἀποσκευὴν τῶν Καρχηδονίων ἅμα τοῖς χρήμασι διήρπαζον , τὸν δὲ Γέσκωνα καὶ τοὺς σὺν αὐτῷ δήσαντες ὑβριστικῶς εἰς φυλακὴν παρεδίδοσαν . καὶ τὸ λοιπὸν ἐπολέμουν ἤδη φανερῶς πρὸς τοὺς Καρχηδονίους , συνωμοσίας ἀσεβεῖς καὶ παρὰ τὰ κοινὰ τῶν ἀνθρώπων ἔθη ποιησάμενοι . ὁ μὲν οὖν πρὸς τοὺς ξένους καὶ Λιβυκὸς ἐπικληθεὶς πόλεμος διὰ ταῦτα καὶ τοιαύτην ἔλαβε τὴν ἀρχήν . οἱ δὲ περὶ τὸν Μάθω συντελεσάμενοι τὰ προειρημένα παραυτίκα μὲν ἐξαπέστελλον πρέσβεις ἐπὶ τὰς κατὰ τὴν Λιβύην πόλεις , παρακαλοῦντες ἐπὶ τὴν ἐλευθερίαν καὶ δεόμενοι σφίσι βοηθεῖν καὶ συνεπιλαμβάνεσθαι τῶν πραγμάτων . μετὰ δὲ ταῦτα πάντων σχεδὸν τῶν κατὰ τὴν Λιβύην ἑτοίμως συνυπακουσάντων αὐτοῖς πρὸς τὴν ἀπὸ τῶν Καρχηδονίων ἀπόστασιν καὶ τάς τε χορηγίας καὶ τὰς βοηθείας προθύμως ἐξαποστελλόντων , διελόντες σφᾶς πολιορκεῖν ἐνεχείρησαν οἱ μὲν τὴν Ἰτύκην , οἱ δὲ τοὺς Ἱππακρίτας , διὰ τὸ ταύτας τὰς πόλεις μὴ βούλεσθαι μετασχεῖν αὐτοῖς τῆς ἀποστάσεως . πολιορκεῖν ἐνεχείρησαν Καρχηδόνιοι δὲ τοὺς μὲν κατ᾽ ἰδίαν βίους ἀεὶ διεξαγαγόντες ἀπὸ τῶν ἐκ τῆς χώρας γεννημάτων , τὰς δὲ κοινὰς παρασκευὰς καὶ χορηγίας ἁθροίζοντες ἐκ τῶν κατὰ τὴν Λιβύην προσόδων , ἔτι δὲ πολεμεῖν εἰθισμένοι ξενικαῖς δυνάμεσι , τότε πάντων ἅμα τούτων οὐ μόνον ἐστερημένοι παραλόγως , ἀλλὰ καὶ καθ᾽ αὑτῶν ὁρῶντες ἕκαστα τῶν προειρημένων ἐπιστρέφοντα , τελέως ἐν μεγάλῃ δυσθυμίᾳ καὶ δυσελπιστίᾳ καθέστασαν , ἅτε παρὰ τὴν προσδοκίαν αὐτοῖς τῶν πραγμάτων ἀποβεβηκότων . τετρυμένοι γὰρ ἐν τῷ περὶ Σικελίας πολέμῳ συνεχῶς ἤλπιζον ἐπιτελεσθεισῶν τῶν διαλύσεων ἀναπνοῆς τινος τεύξεσθαι καὶ καταστάσεως εὐδοκουμένης . συνέβαινε δ᾽ αὐτοῖς τἀναντία : μείζονος γὰρ ἐνίστατο πολέμου καταρχὴ καὶ φοβερωτέρου . πρόσθεν μὲν γὰρ ὑπὲρ Σικελίας ἠμφισβήτουν Ῥωμαίοις , τότε δὲ περὶ σφῶν αὐτῶν καὶ τῆς πατρίδος ἔμελλον κινδυνεύσειν , πόλεμον ἀναλαμβάνοντες ἐμφύλιον . πρὸς δὲ τούτοις οὐχ ὅπλων πλῆθος , οὐ ναυτικὴ δύναμις , οὐ πλοίων κατασκευὴ παρ᾽ αὐτοῖς ἦν , ὡς ἂν τοσαύταις ναυμαχίαις περιπεπτωκότων : καὶ μὴν οὐ -δὲ χορηγιῶν διάθεσις οὐ -δὲ φίλων οὐ -δὲ συμμάχων τῶν βοηθησόντων ἔξωθεν ἐλπὶς οὐ -δ᾽ ἡτισοῦν ὑπῆρχεν . διὸ καὶ τότε σαφῶς ἔγνωσαν ἡλίκην ἔχει διαφορὰν ξενικὸς καὶ διαπόντιος πόλεμος ἐμφυλίου στάσεως καὶ ταραχῆς . οὐχ ἥκιστα δ᾽ αὐτοὶ σφίσι τῶν τοιούτων καὶ τηλικούτων κακῶν ἐγεγόνεισαν αἴτιοι . κατὰ γὰρ τὸν προγεγονότα πόλεμον εὐλόγους ἀφορμὰς ἔχειν ὑπολαμβάνοντες πικρῶς ἐπεστάτησαν τῶν κατὰ τὴν Λιβύην ἀνθρώπων , παραιρούμενοι μὲν τῶν ἄλλων πάντων τῶν καρπῶν τοὺς ἡμίσεις , διπλασίους δὲ ταῖς πόλεσι τοὺς φόρους ἢ πρὶν ἐπιτάττοντες , συγγνώμην δὲ τοῖς ἀπόροις ἢ συμπεριφορὰν οὐδ᾽ ἡντινοῦν ἐπ᾽ οὐδενὶ τῶν πραττομένων διδόντες , θαυμάζοντες δὲ καὶ τιμῶντες τῶν ἀεὶ στρατηγῶν οὐ τοὺς πρᾴως καὶ φιλανθρώπως τῷ πλήθει χρωμένους , ἀλλὰ τοὺς αὐτοῖς μὲν ἑτοιμάζοντας πλείστας χορηγίας καὶ ἐπισκευάς , τοῖς δὲ κατὰ τὴν χώραν πικρότατα χρωμένους , ὧν εἷς ἦν Ἄννων . διδόντες τοιγαροῦν οἱ μὲν ἄνδρες οὐχ οἷον παρακλήσεως πρὸς τὴν ἀπόστασιν , ἀλλ᾽ ἀγγέλου μόνον ἐδεήθησαν : αἱ δὲ γυναῖκες αἱ τὸν πρὸ τοῦ χρόνον ἀπαγομένους περιορῶσαι τοὺς σφετέρους ἄνδρας καὶ γονεῖς πρὸς τὰς εἰσφοράς , τότε συνομνύουσαι κατὰ πόλεις ἐφ᾽ ᾧ μηδὲν κρύψειν τῶν ὑπαρχόντων αὐταῖς , ἀφαιρούμεναι τὸν κόσμον εἰσέφερον ἀπροφασίστως εἰς τοὺς ὀψωνιασμούς . καὶ τοιαύτην παρεσκεύασαν εὐπορίαν τοῖς περὶ τὸν Μάθω καὶ Σπένδιον ὥστε μὴ μόνον διαλῦσαι τὰ προσοφειλόμενα τῶν ὀψωνίων τοῖς μισθοφόροις κατὰ τὰς ἐπαγγελίας , ἃς ἐποιήσαντο πρὸς τὴν ἀπόστασιν , ἀλλὰ καὶ πρὸς τὸ συνεχὲς εὐπορῆσαι χορηγίας .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polybius1-70-79-2017.xml'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' οὕτως οὐδέποτε δεῖ πρὸς τὸ παρὸν μόνον , ἔτι δὲ μᾶλλον πρὸς τὸ μέλλον ἀποβλέπειν ἀεὶ τοὺς ὀρθῶς βουλευομένους . οὐ μὴν ἀλλὰ καίπερ ἐν τοιούτοις κακοῖς ὄντες οἱ Καρχηδόνιοι , προστησάμενοι τὸν Ἄννωνα στρατηγὸν διὰ τὸ δοκεῖν τοῦτον καὶ πρότερον αὐτοῖς τὰ κατὰ τὴν Ἑκατοντάπυλον τῆς Λιβύης καταστρέψασθαι , συνήθροιζον μὲν μισθοφόρους , καθώπλιζον δὲ τοὺς ἐν ταῖς ἡλικίαις τῶν πολιτῶν : ἐγύμναζον δὲ καὶ συνέταττον τοὺς πολιτικοὺς ἱππεῖς : παρεσκεύαζον δὲ καὶ τὰ περιλιπῆ τῶν πλοίων , τριήρεις καὶ πεντηκοντόρους καὶ τὰ μέγιστα τῶν ἀκατίων . οἱ δὲ περὶ τὸν Μάθω , παραγενομένων αὐτοῖς εἰς ἑπτὰ μυριάδας Λιβύων , ἐπιδιελόντες τούτους , ἀσφαλῶς ἐπολιόρκουν τοὺς Ἰτυκαίους καὶ τοὺς Ἱππακρίτας , βεβαίως δὲ τὴν ἐν τῷ Τύνητι στρατοπεδείαν κατεῖχον , ἀποκεκλείκεσαν δὲ τοὺς Καρχηδονίους ἁπάσης τῆς ἐκτὸς Λιβύης . ἡ γὰρ Καρχηδὼν αὐτὴ μὲν ἐν κόλπῳ κεῖται , προτείνουσα καὶ χερρονησίζουσα τῇ θέσει , τὸ μὲν τῇ θαλάττῃ τὸ δέ τι καὶ λίμνῃ περιεχομένη κατὰ τὸ πλεῖστον : περιεχομένη ὁ δὲ συνάπτων ἰσθμὸς αὐτὴν τῇ Λιβύῃ τὸ πλάτος ὡς εἴκοσι καὶ πέντε σταδίων ἐστιν . τούτου δ᾽ ἐπὶ μὲν τοῦ πρὸς τὸ πέλαγος νεύοντος μέρους οὐ μακρὰν ἡ τῶν Ἰτυκαίων κεῖται πόλις , ἐπὶ δὲ θατέρου παρὰ τὴν λίμνην ὁ Τύνης . κεῖται ἐφ᾽ ὧν ἑκατέρων τότε στρατοπεδεύσαντες οἱ μισθοφόροι καὶ διακλείσαντες ἀπὸ τῆς χώρας τοὺς Καρχηδονίους λοιπὸν ἐπεβούλευον αὐτῇ τῇ πόλει , καὶ ποτὲ μὲν ἡμέρας , ποτὲ δὲ καὶ νύκτωρ παραγινόμενοι πρὸς τὸ τεῖχος εἰς φόβους καὶ θορύβους ὁλοσχερεῖς ἐνέβαλλον τοὺς ἔνδον . Ἄννων δὲ περὶ μὲν τὰς παρασκευὰς ἐνδεχομένως ἐγίνετο : καὶ γὰρ ἦν πρὸς τοῦτο τὸ μέρος εὐφυής : ἐξορμήσας δὲ μετὰ τῆς δυνάμεως ἕτερος ἦν : καὶ γὰρ τοῖς καιροῖς ἀστόχως ἐχρῆτο καὶ τοῖς ὅλοις πράγμασιν ἀπείρως καὶ νωθρῶς . ἦν διὸ καὶ τὸ μὲν πρῶτον εἰς Ἰτύκην παραβοηθήσας τοῖς πολιορκουμένοις καὶ καταπληξάμενος τοὺς ὑπεναντίους τῷ πλήθει τῶν θηρίων : εἶχεν γὰρ οὐκ ἐλάττους ἑκατὸν ἐλεφάντων : καὶ μετὰ ταῦτα λαβὼν προτερήματος ἀρχὴν ὁλοσχεροῦς οὕτως ἐχρήσατο κακῶς ὥστε κινδυνεῦσαι προσαπολέσαι καὶ τοὺς πολιορκουμένους . κομίσας γὰρ ἐκ τῆς πόλεως τοὺς καταπέλτας καὶ τὰ βέλη καὶ συλλήβδην ἁπάσας τὰς πρὸς τὴν πολιορκίαν παρασκευὰς καὶ στρατοπεδεύσας πρὸ τῆς πόλεως ἐνεχείρησε προσβάλλειν πρὸς τὸν τῶν ὑπεναντίων χάρακα . τῶν δὲ θηρίων βιασαμένων εἰς τὴν παρεμβολήν , οὐ δυνάμενοι τὸ βάρος οὐ -δὲ τὴν ἔφοδον οἱ πολέμιοι μεῖναι πάντες ἐξέπεσον ἐκ τῆς στρατοπεδείας . καὶ πολλοὶ μὲν αὐτῶν ἀπέθανον τρωθέντες ὑπὸ τῶν θηρίων , τὸ δὲ διασῳζόμενον μέρος πρός τινα λόφον ἐρυμνὸν καὶ σύμφυτον ἔμενεν , πιστεῦον ταῖς ἐξ αὐτῶν τῶν τόπων ἀσφαλείαις . ὁ δ᾽ Ἄννων εἰθισμένος Νομάσι καὶ Λίβυσι πολεμεῖν , οἵτινες ὅταν ἅπαξ ἐγκλίνωσι ποιοῦνται τὴν φυγὴν ἐπὶ δύ᾽ ἡμέρας καὶ τρεῖς ἐκτοπίζοντες αὑτούς , ὑπολαβὼν καὶ τότε πέρας ἔχειν τοῦ πολέμου καὶ νενικηκέναι τοῖς ὅλοις , τῶν μὲν στρατιωτῶν ὠλιγώρησε καὶ καθόλου τῆς παρεμβολῆς , αὐτὸς δ᾽ εἰσελθὼν εἰς τὴν πόλιν ἐγίνετο περὶ τὴν τοῦ σώματος θεραπείαν . οἱ δὲ συμπεφευγότες τῶν μισθοφόρων εἰς τὸν λόφον , σύντροφοι μὲν γεγονότες τῆς Βάρκα τόλμης , συνήθεις δ᾽ ἐκ τῶν κατὰ Σικελίαν ἀγώνων πολλάκις τῆς αὐτῆς ἡμέρας ποτὲ μὲν ὑποχωρεῖν , ποτὲ δὲ πάλιν ἐκ μεταβολῆς ἐγχειρεῖν τοῖς πολεμίοις , καὶ τότε συνιδόντες τὸν μὲν στρατηγὸν ἀπηλλαγμένον εἰς τὴν πόλιν , τοὺς δὲ πολλοὺς διὰ τὸ προτέρημα ῥᾳθυμοῦντας καὶ διαρρέοντας ἐκ τῆς στρατοπεδείας , συστραφέντες ἐπιτίθενται τῷ χάρακι καὶ πολλοὺς μὲν αὐτῶν ἀπέκτειναν , τοὺς δὲ λοιποὺς ἠνάγκασαν φυγεῖν αἰσχρῶς ὑπὸ τὰ τείχη καὶ τὰς πύλας : ἐκυρίευσαν δὲ τῆς ἀποσκευῆς ἁπάσης καὶ τῆς τῶν πολιορκουμένων παρασκευῆς , ἣν Ἄννων πρὸς τοῖς ἄλλοις ἐκκομίσας ἐκ τῆς πόλεως ἐποίησε τοῖς ἐχθροῖς ὑποχείριον . οὐ μόνον δὲ περὶ τοῦτον τὸν καιρὸν οὕτως ἀνεστράφη νωθρῶς , ἀλλὰ καὶ μετ᾽ ὀλίγας ἡμέρας περὶ τὴν καλουμένην Γόρζαν ἀντιστρατοπεδευσάντων αὐτῷ τῶν πολεμίων , λαβὼν καιροὺς δὶς μὲν ἐκ παρατάξεως εἰς τὸ νικᾶν δὶς δ᾽ ἐξ ἐπιθέσεως , ἅτε καὶ στρατοπεδευόντων σύνεγγυς αὐτῷ τῶν ὑπεναντίων , ἀμφοτέρους δοκεῖ τούτους εἰκῇ καὶ παραλόγως προέσθαι . λαβών διόπερ οἱ Καρχηδόνιοι θεωροῦντες αὐτὸν κακῶς χειρίζοντα τὰς πράξεις , Ἀμίλκαν τὸν ἐπικαλούμενον Βάρκαν αὖθις προεστήσαντο , καὶ τοῦτον ἐξέπεμπον εἰς τὸν ἐνεστῶτα πόλεμον στρατηγόν , δόντες ἑβδομήκοντα μὲν ἐλέφαντας καὶ τοὺς ἐπισυνηγμένους τῶν μισθοφόρων καὶ τοὺς ηὐτομοληκότας ἀπὸ τῶν πολεμίων , ἅμα δὲ τούτοις πολιτικοὺς ἱππεῖς καὶ πεζούς , ὥστε τοὺς σύμπαντας εἰς μυρίους ὑπάρχειν . ὃς κατὰ τὴν πρώτην εὐθέως ἔξοδον καταπληξάμενος τῷ παραδόξῳ τῆς ἐπιβολῆς ἥττησε μὲν τὰς ψυχὰς τῶν ὑπεναντίων , ἔλυσε δὲ τὴν τῆς Ἰτύκης πολιορκίαν , ἐφάνη δ᾽ ἄξιος τῶν προγεγονότων ἔργων καὶ τῆς παρὰ τῷ πλήθει προσδοκίας . τὸ δὲ πραχθὲν ἦν ὑπ᾽ αὐτοῦ περὶ τὴν χρείαν ταύτην τοιοῦτον . τῶν γεωλόφων τῶν ἐπιζευγνύντων τὸν αὐχένα τὸν συνάπτοντα τὴν Καρχηδόνα πρὸς τὴν Λιβύην ὄντων δυσβάτων καὶ χειροποιήτους ἐχόντων διεκβολὰς ἐπὶ τὴν χώραν , συνέβαινε τοὺς περὶ τὸν Μάθω πάντας τοὺς διὰ τῶν προειρημένων λόφων εὐκαίρως κειμένους τόπους φυλακαῖς διειληφέναι , πρὸς δὲ τούτοις τοῦ προσαγορευομένου Μακάρα ποταμοῦ διείργοντος κατά τινας τόπους παραπλησίως τὴν ἐπὶ τὴν χώραν τοῖς ἐκ τῆς πόλεως ἔξοδον καὶ διὰ τὸ πλῆθος τοῦ ῥεύματος ἀβάτου κατὰ τὸ πλεῖστον ὑπάρχοντος , μιᾶς δ᾽ οὔσης ἐπ᾽ αὐτῷ γεφύρας , καὶ ταύτην τηρεῖν τὴν δίοδον ἀσφαλῶς , πόλιν ἐπ᾽ αὐτῆς ᾠκοδομηκότας .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polybius1-70-79-2017.xml'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' ἐξ ὧν συνέβαινε τοὺς Καρχηδονίους μὴ οἷον στρατοπέδῳ τῆς χώρας ἐπιβαίνειν , ἀλλὰ μηδὲ τοὺς κατ᾽ ἰδίαν θέλοντας διαπεσεῖν ῥᾳδίως ἂν δύνασθαι λαθεῖν τοὺς ὑπεναντίους . εἰς ἃ βλέπων Ἀμίλκας καὶ παντὸς πράγματος καὶ καιροῦ πεῖραν λαμβάνων διὰ τὸ δυσχρηστεῖν περὶ τὴν ἔξοδον διενοήθη τι τοιοῦτον . τοῦ προειρημένου ποταμοῦ κατὰ τὴν εἰς θάλατταν ἐκβολὴν συνθεωρήσας κατά τινας ἀνέμων στάσεις ἀποθινούμενον τὸ στόμα καὶ τεναγώδη γινομένην τὴν παρ᾽ αὐτὸ τὸ στόμα πάροδον , ποιήσας εὐτρεπῆ τῷ στρατοπέδῳ τὰ πρὸς τὴν ἔξοδον καὶ κρύπτων ἐν αὑτῷ τὴν ἐπιβολὴν ἐτήρει τὸ προειρημένον σύμπτωμα . παραπεσόντος δὲ τοῦ καιροῦ , νυκτὸς ἐξορμήσας ἔλαθε πάντας ἅμα τῷ φωτὶ τὸν προειρημένον τόπον διαβιβάσας τὴν δύναμιν . παραδόξου δὲ τοῦ πράγματος φανέντος καὶ τοῖς ἐν τῇ πόλει καὶ τοῖς ὑπεναντίοις , ὁ μὲν Ἀμίλκας προῆγεν διὰ τοῦ πεδίου , ποιούμενος τὴν πορείαν ἐπὶ τοὺς τὴν γέφυραν φυλάττοντας . οἱ δὲ περὶ τὸν Σπένδιον συνέντες τὸ γεγονὸς ἀπήντων εἰς τὸ πεδίον καὶ παρεβοήθουν ἀλλήλοις , οἱ μὲν ἐκ τῆς περὶ τὴν γέφυραν πόλεως ὄντες οὐκ ἐλάττους μυρίων , οἱ δ᾽ ἀπὸ τῆς Ἰτύκης ὑπὲρ τοὺς μυρίους καὶ πεντακισχιλίους . ὄντες ἐπεὶ δ᾽ εἰς σύνοπτον ἧκον ἀλλήλοις , νομίσαντες ἐν μέσῳ τοὺς Καρχηδονίους ἀπειληφέναι , σπουδῇ παρηγγύων ἅμα παρακαλοῦντες σφᾶς αὐτοὺς καὶ συνῆπτον τοῖς πολεμίοις . ὁ δ᾽ Ἀμίλκας ἦγε μὲν τὴν πορείαν πρώτους ἔχων τοὺς ἐλέφαντας , ἐπὶ δὲ τούτοις τοὺς ἱππεῖς καὶ τοὺς εὐζώνους , τελευταῖα δὲ τὰ βαρέα τῶν ὅπλων . ἔχων κατιδὼν δὲ προχειρότερον ἐπιφερομένους τοὺς ὑπεναντίους , ἀναστρέφειν παρήγγειλε πᾶσι τοῖς ἑαυτοῦ . καὶ τοὺς μὲν ἀπὸ τῆς πρωτοπορείας ἀναστρέψαντας σπουδῇ ποιεῖσθαι τὴν ἀποχώρησιν ἐκέλευσε : τοὺς δ᾽ ἐπὶ τῆς οὐραγίας ἐξ ἀρχῆς ὑπάρχοντας ἐξ ἐπιστροφῆς περισπῶν ἐξέταττε πρὸς τὴν τῶν πολεμίων ἐπιφάνειαν . οἱ δὲ Λίβυες καὶ μισθοφόροι νομίσαντες αὐτοὺς καταπεπληγμένους φυγεῖν , λύσαντες τὴν τάξιν ἐπέκειντο καὶ συνῆπτον εἰς τὰς χεῖρας ἐρρωμένως , ἅμα δὲ τῷ τοὺς ἱππεῖς συνεγγίσαντας τοῖς παρατεταγμένοις ἐκ μεταβολῆς ὑποστῆναι , τὴν δὲ λοιπὴν δύναμιν ἐπάγειν , ἐκπλαγεῖς γινόμενοι διὰ τὸ παράδοξον οἱ Λίβυες ἐγκλίναντες εὐθέως ἔφευγον , ὡς ἂν εἰκῇ καὶ σποράδην ἐπικείμενοι . λοιπὸν οἱ μὲν τοῖς κατόπιν ἐπιφερομένοις περιπίπτοντες ἐσφάλλοντο καὶ διέφθειρον αὑτούς τε καὶ τοὺς οἰκείους : οἱ δὲ πλείους συνεπατήθησαν , ἐκ χειρὸς τῶν ἱππέων ἐπικειμένων αὐτοῖς καὶ τῶν θηρίων . ἀπώλοντο μὲν οὖν εἰς ἑξακισχιλίους τῶν Λιβύων καὶ τῶν ξένων , ἑάλωσαν δὲ περὶ δισχιλίους : οἱ δὲ λοιποὶ διέφυγον , οἱ μὲν εἰς τὴν πρὸς τῇ γεφύρᾳ πόλιν , οἱ δ᾽ ἐπὶ τὴν πρὸς Ἰτύκῃ παρεμβολήν . διέφυγον διέφυγον Ἀμίλκας δὲ ποιήσας τὸ προτέρημα τὸν προειρημένον τρόπον , εἵπετο κατὰ πόδας τοῖς πολεμίοις καὶ τὴν μὲν ἐπὶ τῆς γεφύρας πόλιν ἐξ ἐφόδου κατέσχεν , προεμένων καὶ φευγόντων εἰς τὸν Τύνητα τῶν ἐν αὐτῇ πολεμίων , τὴν δὲ λοιπὴν χώραν ἐπιπορευόμενος τὰς μὲν προσήγετο , πλείστας δὲ κατὰ κράτος ἐξῄρει . τοῖς δὲ Καρχηδονίοις βραχύ τι θάρσους ἐνειργάσατο καὶ τόλμης , ἐπὶ ποσὸν αὐτοὺς ἀπαλλάξας τῆς προγεγενημένης δυσελπιστίας . ὁ δὲ Μάθως αὐτὸς μὲν ἐπὶ τῆς τῶν Ἱππακριτῶν πολιορκίας ἐπέμενεν , τοῖς δὲ περὶ τὸν Αὐτάριτον τὸν τῶν Γαλατῶν ἡγεμόνα καὶ Σπένδιον ἔχεσθαι τῶν ὑπεναντίων συνεβούλευε , τὰ μὲν πεδία φεύγοντας διὰ τὸ πλῆθος τῶν παρὰ τοῖς ὑπεναντίοις ἱππέων καὶ θηρίων , ταῖς δ᾽ ὑπωρείαις ἀντιπαράγοντας καὶ συνεπιτιθεμένους κατὰ τὰς ὑποπιπτούσας ἀεὶ δυσχερείας . ἅμα δὲ ταῖς ἐπινοίαις ταύταις καὶ πρὸς τοὺς Νομάδας καὶ τοὺς Λίβυας ἐξέπεμπε , δεόμενος βοηθεῖν σφίσι καὶ μὴ καταπροΐεσθαι τοὺς ὑπὲρ τῆς ἐλευθερίας καιρούς . ὁ δὲ Σπένδιος προσλαβὼν ἐκ τοῦ Τύνητος ἀφ᾽ ἑκάστου τῶν γενῶν τοὺς πάντας εἰς ἑξακισχιλίους προῆγε , ταῖς ὑπωρείαις ἀντιπαράγων τοῖς Καρχηδονίοις , ἔχων ἅμα τοῖς προειρημένοις καὶ τοὺς μετ᾽ Αὐταρίτου Γαλάτας , ὄντας εἰς δισχιλίους .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polybius1-70-79-2017.xml'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' τὸ γὰρ λοιπὸν μέρος αὐτῶν τοῦ κατ᾽ ἀρχὰς συστήματος ηὐτομολήκει πρὸς τοὺς Ῥωμαίους ἐν ταῖς περὶ τὸν Ἔρυκα στρατοπεδείαις . τοῦ δ᾽ Ἀμίλκου παρεμβεβληκότος ἔν τινι πεδίῳ πανταχόθεν ὄρεσι περιεχομένῳ , συνέβη τὰς παρὰ τῶν Νομάδων καὶ Λιβύων βοηθείας εἰς τὸν καιρὸν τοῦτον συνάψαι τοῖς περὶ τὸν Σπένδιον . γενομένης δὲ τοῖς Καρχηδονίοις τῆς μὲν τῶν Λιβύων ἐπιστρατοπεδείας αἰφνιδίου καὶ κατὰ πρόσωπον , τῆς δὲ τῶν Νομάδων ἀπ᾽ οὐρᾶς , τῆς δὲ περὶ τὸν Σπένδιον ἐκ πλαγίου , μεγάλην αὐτοῖς ἀπορίαν συνέβη περιστῆναι καὶ δυσέκφευκτον . γενομένης κατὰ δὲ τὸν καιρὸν τοῦτον Ναραύας , ὃς ἦν μὲν Νομὰς τῶν ἐνδοξοτάτων εἷς , ἦν δὲ καὶ πλήρης ὁρμῆς πολεμικῆς , οὗτος ἀεὶ μὲν οἰκείως διέκειτο πρὸς τοὺς Καρχηδονίους πατρικὴν ἔχων σύστασιν , τότε δὲ μᾶλλον παρωρμήθη διὰ τὴν Ἀμίλκου τοῦ στρατηγοῦ καταξίωσιν . διὸ καὶ νομίσας ἔχειν εὐφυῆ καιρὸν πρὸς ἔντευξιν αὐτῷ καὶ σύστασιν , ἧκεν εἰς τὴν στρατοπεδείαν ἔχων περὶ αὑτὸν Νομάδας εἰς ἑκατόν . καὶ συνεγγίσας τῷ χάρακι τολμηρῶς ἔμενε , κατασείων τῇ χειρί . τοῦ δ᾽ Ἀμίλκου θαυμάσαντος τὴν ἐπιβολὴν καὶ προπέμψαντός τινα τῶν ἱππέων , εἰς λόγους ἔφη βούλεσθαι συνελθεῖν τῷ στρατηγῷ . διαποροῦντος δ᾽ ἀκμὴν καὶ διαπιστοῦντος τοῦ τῶν Καρχηδονίων ἡγεμόνος , παραδοὺς ὁ Ναραύας τὸν ἵππον καὶ τὰς λόγχας τοῖς μεθ᾽ αὑτοῦ παρῆν ἄνοπλος εὐθαρσῶς εἰς τὴν παρεμβολήν . οἱ δὲ τὰ μὲν ἐθαύμαζον , τὰ δὲ κατεπλήττοντο τὴν τόλμαν : ὅμως δὲ προσεδέξαντο καὶ συνῆλθον εἰς τὰς χεῖρας . ὁ δὲ παραγενόμενος εἰς λόγους ἔφη πᾶσι μὲν Καρχηδονίοις εὐνοεῖν , μάλιστα δ᾽ ἐπιθυμεῖν Βάρκᾳ γενέσθαι φίλος : διὸ καὶ νῦν παρεῖναι συσταθησόμενος αὐτῷ καὶ κοινωνήσων ἀδόλως παντὸς ἔργου καὶ πάσης ἐπιβολῆς . Ἀμίλκας δὲ ταῦτ᾽ ἀκούσας οὕτως ἥσθη μεγάλως ἐπί τε τῷ κατὰ τὴν παρουσίαν θάρσει καὶ τῇ κατὰ τὴν ἔντευξιν ἁπλότητι τοῦ νεανίσκου , ὡς οὐ μόνον εὐδόκησε κοινωνὸν αὐτὸν προσλαβέσθαι τῶν πράξεων , ἀλλὰ καὶ τὴν θυγατέρα δώσειν ἐπηγγείλατο μεθ᾽ ὅρκου , διαφυλάξαντος αὐτοῦ τὴν πρὸς Καρχηδονίους πίστιν . γενομένων δὲ τῶν ὁμολογιῶν , ὁ μὲν Ναραύας ἧκε τοὺς ὑφ᾽ αὑτὸν τεταγμένους ἔχων Νομάδας , ὄντας εἰς δισχιλίους , ὁ δ᾽ Ἀμίλκας προσγενομένης αὐτῷ τῆς χειρὸς ταύτης παρετάξατο τοῖς πολεμίοις . οἱ δὲ περὶ τὸν Σπένδιον συνάψαντες ἐπὶ ταὐτὸ τοῖς Λίβυσι καὶ καταβάντες εἰς τὸ πεδίον συνέβαλλον τοῖς Καρχηδονίοις . γενομένης δὲ μάχης ἰσχυρᾶς ἐνίκων οἱ περὶ τὸν Ἀμίλκαν , καλῶς μὲν τῶν θηρίων ἀγωνισαμένων , ἐπιφανεστάτην δὲ τοῦ Ναραύα παρασχομένου χρείαν . ὁ μὲν οὖν Αὐτάριτος καὶ Σπένδιος διέφυγον , τῶν δὲ λοιπῶν ἔπεσον μὲν εἰς μυρίους , ἑάλωσαν δ᾽ εἰς τετρακισχιλίους . ἐπιτελεσθέντος δὲ τοῦ κατορθώματος , Ἀμίλκας τοῖς μὲν βουλομένοις τῶν αἰχμαλώτων μεθ᾽ ἑαυτοῦ συστρατεύειν ἐξουσίαν ἔδωκε καὶ καθώπλιζε τοῖς ἀπὸ τῶν πολεμίων σκύλοις , τοὺς δὲ μὴ βουλομένους ἁθροίσας παρεκάλει φάσκων , ἕως μὲν τοῦ νῦν συγγνώμην αὐτοῖς ἔχειν τῶν ἡμαρτημένων : διὸ καὶ συγχωρεῖν τρέπεσθαι κατὰ τὰς ἰδίας ὁρμὰς οὗ ποτ᾽ ἂν ἕκαστος αὐτῶν προαιρῆται . μετὰ δὲ ταῦτα διηπειλήσατο μηθένα φέρειν ὅπλον πολέμιον κατ᾽ αὐτῶν , ὡς , ἐὰν ἁλῷ τις , ἀπαραιτήτου τευξόμενον τιμωρίας . κατὰ δὲ τοὺς αὐτοὺς καιροὺς οἱ τὴν Σαρδόνα [τὴν νῆσον] παραφυλάττοντες τῶν μισθοφόρων , ζηλώσαντες τοὺς περὶ τὸν Μάθω καὶ Σπένδιον , ἐπιτίθενται τοῖς ἐν τῇ νήσῳ Καρχηδονίοις .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polybius1-70-79-2017.xml'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' καὶ τὸν μὲν τότε παρ᾽ αὐτοῖς ὄντα βοήθαρχον Βώσταρον συγκλείσαντες εἰς τὴν ἀκρόπολιν μετὰ τῶν ἑαυτοῦ πολιτῶν ἀπέκτειναν . αὖθις δὲ τῶν Καρχηδονίων στρατηγὸν ἐξαποστειλάντων μετὰ δυνάμεως Ἄννωνα , κἄπειτα καὶ τούτων τῶν δυνάμεων ἐγκαταλιπουσῶν τὸν Ἄννωνα καὶ μεταθεμένων πρὸς σφᾶς , γενόμενοι ζωγρίᾳ κύριοι τοῦ προειρημένου , παραυτίκα τοῦτον μὲν ἀνεσταύρωσαν , μετὰ δὲ ταῦτα παρηλλαγμένας ἐπινοοῦντες τιμωρίας πάντας τοὺς ἐν τῇ νήσῳ Καρχηδονίους στρεβλοῦντες ἀπέκτειναν : καὶ τὸ λοιπὸν ἤδη ποιησάμενοι τὰς πόλεις ὑφ᾽ ἑαυτοὺς εἶχον ἐγκρατῶς τὴν νῆσον , ἕως οὗ στασιάσαντες πρὸς τοὺς Σαρδονίους ἐξέπεσον ὑπ᾽ ἐκείνων εἰς τὴν Ἰταλίαν . ἡ μὲν οὖν Σαρδὼ τοῦτον τὸν τρόπον ἀπηλλοτριώθη Καρχηδονίων , νῆσος καὶ τῷ μεγέθει καὶ τῇ πολυανθρωπίᾳ καὶ τοῖς γεννήμασι διαφέρουσα . τῷ δὲ πολλοὺς καὶ πολὺν ὑπὲρ αὐτῆς πεποιῆσθαι λόγον οὐκ ἀναγκαῖον ἡγούμεθ᾽ εἶναι ταυτολογεῖν ὑπὲρ τῶν ὁμολογουμένων . Μάθως δὲ καὶ Σπένδιος , ἅμα δὲ τούτοις Αὐτάριτος ὁ Γαλάτης ὑπιδόμενοι τὴν Ἀμίλκου φιλανθρωπίαν εἰς τοὺς αἰχμαλώτους καὶ φοβηθέντες μὴ τῷ τοιούτῳ τρόπῳ ψυχαγωγηθέντες ὁρμήσωσι πρὸς τὴν ὑποδεικνυμένην ἀσφάλειαν οἵ τε Λίβυες καὶ τὸ τῶν μισθοφόρων πλῆθος , ἐβουλεύοντο πῶς ἂν καινοτομήσαντές τι τῶν πρὸς ἀσέβειαν εἰς τέλος ἀποθηριώσειαν τὰ πλήθη πρὸς τοὺς Καρχηδονίους . ἔδοξεν οὖν αὐτοῖς συναθροῖσαι τοὺς πολλούς . γενομένου δὲ τούτου γραμματοφόρον εἰσήγαγον , ὡς ἀπεσταλμένον ὑπὸ τῶν ἐκ τῆς Σαρδόνος αἱρετιστῶν . ἡ δ᾽ ἐπιστολὴ διεσάφει τόν τε Γέσκωνα καὶ τοὺς μετ᾽ αὐτοῦ πάντας , οὓς παρεσπόνδησαν ἐν τῷ Τύνητι , καθάπερ ἐπάνω προεῖπον , φυλάττειν ἐπιμελῶς , ὡς πραττόντων τινῶν ἐκ τοῦ στρατοπέδου τοῖς Καρχηδονίοις ὑπὲρ τῆς τούτων σωτηρίας . λαβόμενος δὲ τῆς ἀφορμῆς ταύτης ὁ Σπένδιος πρῶτον μὲν παρεκάλει μὴ πιστεύειν τὴν ὑπὸ τοῦ στρατηγοῦ τοῦ τῶν Καρχηδονίων γεγενημένην φιλανθρωπίαν πρὸς τοὺς αἰχμαλώτους : οὐ γὰρ σῶσαι προαιρούμενον αὐτὸν ταῦτα βεβουλεῦσθαι περὶ τῶν ἁλόντων , ἀλλὰ διὰ τῆς ἐκείνων ἀφέσεως ἡμῶν ἐγκρατῆ γενέσθαι σπουδάζοντα , πρὸς τὸ μὴ τινὰς ἀλλὰ πάντας ἡμᾶς ἅμα τιμωρήσασθαι πιστεύσαντας αὐτῷ . πρὸς δὲ τούτοις φυλάττεσθαι παρῄνει μὴ προέμενοι τοὺς περὶ τὸν Γέσκωνα καταφρονηθῶσι μὲν ὑπὸ τῶν ἐχθρῶν , μεγάλα δὲ βλάψωσι τὰς ἰδίας πράξεις , ἄνδρα τοιοῦτον καὶ στρατηγὸν ἀγαθὸν ἐάσαντες διαφυγεῖν , ὃν εἰκὸς ἐχθρὸν αὐτοῖς ἔσεσθαι φοβερώτατον . ἔτι δὲ ταῦτα λέγοντος αὐτοῦ παρῆν ἄλλος γραμματοφόρος , ὡς ἀπὸ τῶν ἐκ τοῦ Τύνητος ἀπεσταλμένος , παραπλήσια τοῖς ἐκ τῆς Σαρδόνος διασαφῶν ..'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stampare i vari chunks in celle separate  \n",
    "from IPython.display import display\n",
    "num = 117\n",
    "for i in range(len(all_opera_split[opera_names[num]])):\n",
    "    display(opera_names[num])\n",
    "    display(all_opera_split[opera_names[num]][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_lis = os.listdir('output_gorman_no_anomalies')\n",
    "\n",
    "# Lista per salvare tutti i percorsi dei file CSV\n",
    "csv_file_paths = []\n",
    "\n",
    "# Directory principale che contiene le sottodirectory\n",
    "main_directory = 'output_gorman_no_anomalies'\n",
    "\n",
    "#per ogni cartella in output gorman devo ottenere la directory completa della singola cartella dell'autore. per farlo itero col ciclo for su tutte le cartelle presenti in gorman e concateno,\n",
    "#per ottenere la directory della cartella della singola opera, la main directory col nome della cratella.\n",
    "#per farlo uso la funzione di os che si chiama os.path join che fa una concatenazione di stringhe.\n",
    "for folder in dir_lis:\n",
    "  folder_path = os.path.join(main_directory, folder)\n",
    "  #check is folder_path is folder dir\n",
    "  if os.path.isdir(folder_path):  #ora voglio per ogni cartella una lista vuota poi da andare a riempire con i csv di ogni singola cartella.\n",
    "    singola_cartella = []\n",
    "    for file in os.listdir(folder_path): #per ogni file che sta nella mia cartella\n",
    "      if file.endswith('.csv'): #controllo per vedere se sono davvero tutti csv i file.\n",
    "        file_path = os.path.join(folder_path, file) #se il file è un csv la sua directory completa me la metti nella lista.\n",
    "        singola_cartella.append(file_path)\n",
    "\n",
    "  csv_file_paths.append(singola_cartella)\n",
    "csv_file_paths = sorted(csv_file_paths)\n",
    "# Lista per conservare i DataFrame filtrati\n",
    "#per ogni lista creo una lista di di dataframe. questi sono i csv filtrati con le info che mi servono.\n",
    "#ora lo trasformo in una funzione che prenda in input una lista di csv.\n",
    "def filter_dataframe(csv_file_paths):\n",
    "  dataframes_tot = [] #è una lista vuota in cui mettere i dataframe filtrati.\n",
    "# Leggi, filtra e aggiungi ogni DataFrame alla lista\n",
    "  for file_path in csv_file_paths:\n",
    "    # Leggi il CSV in un DataFrame\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filtra il DataFrame mantenendo solo le righe dove '@lemma' è una stringa\n",
    "      df_filtered = df[df['@lemma'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    # Aggiungi il DataFrame filtrato alla lista\n",
    "      dataframes_tot.append(df_filtered)\n",
    "\n",
    "# Visualizza le prime righe di ciascun DataFrame per controllare la struttura\n",
    "      data_info = {f\"File {i+1}\": df.head() for i, df in enumerate(dataframes_tot)}\n",
    "\n",
    "# Mostra la struttura dei dati\n",
    "      #for file_name, df_head in data_info.items():\n",
    "       # print(f\"\\n{file_name}:\")\n",
    "        #print(df_head)\n",
    "  return dataframes_tot\n",
    "# Function to extract the numeric part from the filename\n",
    "def extract_number(file_path):\n",
    "    # Use regex to find the numeric part of the filename\n",
    "    return int(re.search(r'(\\d+)\\.csv$', file_path).group(1))\n",
    "all_sorted_csv_filepaths = []\n",
    "for folder in csv_file_paths:\n",
    "  #print(folder)\n",
    "  sorted_files = sorted(folder, key=extract_number)\n",
    "  all_sorted_csv_filepaths.append(sorted_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui è solo GORMAN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [filter_dataframe(lis) for lis in all_sorted_csv_filepaths] #Filtra il DataFrame mantenendo solo le righe dove '@lemma' è una stringa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenazione dei dataframes di un'opera in un unico dataframe \n",
    "#nell'esempio abbiamo la prima opera. così in una tabella ho tutta l'opera e non frase per frase. \n",
    "\n",
    "#per farli tutti e metterli in una lista, è una lista in cui ogni dataframe corrisponde a tutta una singola opera es. eschine 1-50 è un'opera:\n",
    "dataframes_unificati = []\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes_unificati.append(pd.concat(dataframes[i],ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframe_of_lemmas_postags(dataframes_unificati):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame in cui ogni riga corrisponde a una form e le colonne\n",
    "    sono form, lemma1, lemma2, ..., postag1, postag2, ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframes_unificati : list of pd.DataFrame\n",
    "        Lista di DataFrame da concatenare.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Un DataFrame con colonne:\n",
    "        ['form', 'lemma1', 'lemma2', ..., 'postag1', 'postag2', ...]\n",
    "    \"\"\"\n",
    "    # Concatena i DataFrame e seleziona le colonne di interesse\n",
    "    df_per_lemma = pd.concat(dataframes_unificati, ignore_index=True)[\n",
    "        ['@id','@form','@lemma','@postag','@relation']\n",
    "    ]\n",
    "    \n",
    "    # Creiamo un dizionario temporaneo per raccogliere i lemmi e i postag unici per form\n",
    "    lemma_dict = {}\n",
    "    # Ricaviamo la lista delle form uniche (mantiene l'ordine di apparizione)\n",
    "    forms = df_per_lemma['@form'].dropna().unique()\n",
    "    \n",
    "    for form in forms:\n",
    "        # Estrai tutti i lemmi e i postag unici corrispondenti a questa form\n",
    "        lemmas = (\n",
    "            df_per_lemma.loc[df_per_lemma['@form'] == form, '@lemma']\n",
    "            .dropna()\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        postags = (\n",
    "            df_per_lemma.loc[df_per_lemma['@form'] == form, '@postag']\n",
    "            .dropna()\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        lemma_dict[form] = {'lemmas': lemmas, 'postags': postags}\n",
    "    \n",
    "    # Troviamo quanti sono i lemmi massimi e i postag massimi associati a una form\n",
    "    max_lemmas = max(len(info['lemmas']) for info in lemma_dict.values())\n",
    "    max_postags = max(len(info['postags']) for info in lemma_dict.values())\n",
    "    \n",
    "    # Creiamo la lista delle colonne: form, lemma1..lemmaN, postag1..postagM\n",
    "    columns = (['form']\n",
    "               + [f'lemma{i}' for i in range(1, max_lemmas + 1)]\n",
    "               + [f'postag{i}' for i in range(1, max_postags + 1)])\n",
    "    \n",
    "    # Creiamo le righe del dataframe\n",
    "    rows = []\n",
    "    for form, info in lemma_dict.items():\n",
    "        row = {'form': form}\n",
    "        \n",
    "        # Inseriamo lemma1, lemma2, ...\n",
    "        for i, lemma in enumerate(info['lemmas'], start=1):\n",
    "            row[f'lemma{i}'] = lemma\n",
    "        # Per i lemmi mancanti, se vuoi puoi lasciarli a None (il dict non li avrà, ma la riga verrà comunque allineata con reindex)\n",
    "        \n",
    "        # Inseriamo postag1, postag2, ...\n",
    "        for i, postag in enumerate(info['postags'], start=1):\n",
    "            row[f'postag{i}'] = postag\n",
    "        # Stessa logica per eventuali postag mancanti.\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Convertiamo in DataFrame\n",
    "    final_df = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "# ------------------ Esempio d'uso -------------------\n",
    "\n",
    "# Supponiamo di avere già una lista di DataFrame\n",
    "# dataframes_unificati = [df1, df2, ...]\n",
    "\n",
    "# Creiamo il DataFrame finale\n",
    "# df_lemmas_postags = create_dataframe_of_lemmas_postags(dataframes_unificati)\n",
    "# print(df_lemmas_postags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pos_dic_gorman = create_dataframe_of_lemmas_postags(dataframes_unificati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pos_dic_gorman.to_csv(\"lemmas_postags.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forse avrei dovuto usare output_gorman_con_relation, qui ho usato i no anomalies dai quali avevo già corretto alcune anomalie di postag!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the column values into one string separated by spaces\n",
    "for i,df in enumerate(dataframes_unificati):\n",
    "    text = \" \".join(df['@form'].astype(str))\n",
    "\n",
    "    # Write the string to a text file\n",
    "    with open('output_treetagger_baseline/'+opera_names[i]+'.txt', \"w\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUI INVECE STO CARICANDO E PULENDO I LEMMI DI CHATGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check per vedere se i lemma di gpt e di gorman sono uguali, noi abbiamo tenuto solo quelli diversi. \n",
    "\n",
    "def remove_unwanted_rows(df):  \n",
    "    \"\"\"\n",
    "    Removes rows based on specific conditions, but ensures rows with '@form' != '@form' are not removed.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Replace empty strings with NaN\n",
    "    df = df.replace('', pd.NA)\n",
    "    \n",
    "    # Base condition: rows where '@form' is not '@form' should always be kept\n",
    "    conditions_to_remove = (df['@form'] == '@form')\n",
    "\n",
    "    # Check for additional columns dynamically\n",
    "    if '@lemmaPerseus2' in df.columns:\n",
    "        # Case 1: Both '@lemmaPerseus1' and '@lemmaPerseus2' are NaN\n",
    "        conditions_to_remove &= (df['@lemmaPerseus1'].isna()) & (df['@lemmaPerseus2'].isna())\n",
    "        \n",
    "        # Case 2: Both '@lemmaPerseus1' and '@lemmaPerseus2' are not NaN\n",
    "        conditions_to_remove |= (df['@lemmaPerseus1'].notna()) & (df['@lemmaPerseus2'].notna())\n",
    "        \n",
    "        if '@lemmaPerseus3' in df.columns:\n",
    "            # Case 3: '@lemmaPerseus2' and '@lemmaPerseus3' are both NaN\n",
    "            conditions_to_remove |= (df['@lemmaPerseus2'].isna()) & (df['@lemmaPerseus3'].isna())\n",
    "            \n",
    "            # Case 4: '@lemmaPerseus3' is not NaN\n",
    "            conditions_to_remove |= df['@lemmaPerseus3'].notna()\n",
    "            \n",
    "            # Case 5: All three are NaN\n",
    "            conditions_to_remove |= (\n",
    "                df['@lemmaPerseus1'].isna() &\n",
    "                df['@lemmaPerseus2'].isna() &\n",
    "                df['@lemmaPerseus3'].isna()\n",
    "            )\n",
    "    elif '@lemmaPerseus3' in df.columns:\n",
    "        # If only '@lemmaPerseus3' exists, handle it alone\n",
    "        conditions_to_remove &= df['@lemmaPerseus3'].notna()\n",
    "\n",
    "    # Ensure that rows where '@form' != '@form' are never removed\n",
    "    filtered_df = df[~(conditions_to_remove & (df['@form'] == '@form'))]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_dataframe_with_postags(df1, df2):\n",
    "    \"\"\"\n",
    "    Merges two DataFrames into a structured DataFrame containing:\n",
    "    - '@form': The form (key).\n",
    "    - Lemmas from df1 ('gorman_lemma_X').\n",
    "    - Lemmas from df2 ('gpt_lemma_X').\n",
    "    - Postags from df1 ('postag_X').\n",
    "\n",
    "    Args:\n",
    "    df1 (pd.DataFrame): The first DataFrame containing '@form', '@lemma', and '@postag'.\n",
    "    df2 (pd.DataFrame): The second DataFrame containing '@form' and '@lemmaPerseus1', '@lemmaPerseus2', '@lemmaPerseus3'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary\n",
    "    lemma_dict = {}\n",
    "\n",
    "    # Get unique forms from both DataFrames\n",
    "    forms = list(dict.fromkeys(list(df1['@form']) +  list(df2['@form'])))\n",
    "\n",
    "    for form in forms:\n",
    "        # Extract lemmas from df1\n",
    "        lemmas_df1 = df1.loc[df1['@form'] == form, '@lemma'].dropna().unique().tolist()\n",
    "\n",
    "        # Extract lemmas from df2\n",
    "        lemmas_df2 = (\n",
    "            df2.loc[\n",
    "                df2['@form'] == form, ['@lemmaPerseus1', '@lemmaPerseus2']\n",
    "            ]\n",
    "            .stack()\n",
    "            .dropna()\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Extract unique @postag values for the form from df1\n",
    "        postags = df1.loc[df1['@form'] == form, '@postag'].dropna().unique().tolist()\n",
    "\n",
    "        # Add to the dictionary\n",
    "        lemma_dict[form] = {\n",
    "            'lemmas_df1': lemmas_df1,\n",
    "            'lemmas_df2': lemmas_df2,\n",
    "            'postags': postags\n",
    "        }\n",
    "\n",
    "    # Determine the maximum lengths\n",
    "    max_len_df1 = max(len(lemmas['lemmas_df1']) for lemmas in lemma_dict.values())\n",
    "    max_len_df2 = max(len(lemmas['lemmas_df2']) for lemmas in lemma_dict.values())\n",
    "    max_len_postags = max(len(lemmas['postags']) for lemmas in lemma_dict.values())\n",
    "\n",
    "    # Create the columns for the DataFrame\n",
    "    columns = (\n",
    "        ['@form'] +\n",
    "        [f'gorman_lemma_{i+1}' for i in range(max_len_df1)] +\n",
    "        [f'gpt_lemma_{i+1}' for i in range(max_len_df2)] +\n",
    "        [f'postag_{i+1}' for i in range(max_len_postags)]\n",
    "    )\n",
    "\n",
    "    # Build the DataFrame rows\n",
    "    data = []\n",
    "    for form, lemmas in lemma_dict.items():\n",
    "        row = {'@form': form}\n",
    "        # Add lemmas from df1 (gorman), filling with empty strings if necessary\n",
    "        for i in range(max_len_df1):\n",
    "            row[f'gorman_lemma_{i+1}'] = lemmas['lemmas_df1'][i] if i < len(lemmas['lemmas_df1']) else ''\n",
    "        # Add lemmas from df2 (gpt), filling with empty strings if necessary\n",
    "        for i in range(max_len_df2):\n",
    "            row[f'gpt_lemma_{i+1}'] = lemmas['lemmas_df2'][i] if i < len(lemmas['lemmas_df2']) else ''\n",
    "        # Add postags from df1, filling with empty strings if necessary\n",
    "        for i in range(max_len_postags):\n",
    "            row[f'postag_{i+1}'] = lemmas['postags'][i] if i < len(lemmas['postags']) else ''\n",
    "        data.append(row)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    merged_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"merged_df.to_csv('eschine_con_ripetizioni_1/50_all_lemma_and_pos_gorman/gpt.csv')\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''merged_df.to_csv('eschine_con_ripetizioni_1/50_all_lemma_and_pos_gorman/gpt.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"concatenated_df = pd.concat([df1, lemma_chat_eschine_1_50], axis=1) #per vedere dove chat gpt ha sbagliato, cioè ha sbagliato le crasi, le ha unite. se due forme erano ou-de le ha unite in oude\\nconcatenated_df.to_csv('dove_gpt_ha_sbagliato_eschine.csv')\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpt non fa la stessa lemmatizzazione di gorman perchè al netto della punteggiatura tolta unifica le crasi. Non riesco a unificare le due tabelle senza problemi.\n",
    "'''concatenated_df = pd.concat([df1, lemma_chat_eschine_1_50], axis=1) #per vedere dove chat gpt ha sbagliato, cioè ha sbagliato le crasi, le ha unite. se due forme erano ou-de le ha unite in oude\n",
    "concatenated_df.to_csv('dove_gpt_ha_sbagliato_eschine.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_equal_lemmas(df):\n",
    "    \"\"\"\n",
    "    Rimuove le righe in cui:\n",
    "    - Come lemmi non vuoti ci sono solo `gorman_lemma_1` e `gpt_lemma_1`.\n",
    "    - `gorman_lemma_1` e `gpt_lemma_1` sono uguali.\n",
    "    - I `postag` non influiscono sulla condizione.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Il DataFrame di input.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Il DataFrame filtrato.\n",
    "    \"\"\"\n",
    "    df = df.replace('', np.nan)\n",
    "    # Condizione 1: Come lemmi non vuoti ci sono solo `gorman_lemma_1` e `gpt_lemma_1`\n",
    "    only_first_lemmas_non_empty = (\n",
    "        df[[col for col in df.columns if col.startswith('gorman_lemma')]].notna().sum(axis=1) == 1\n",
    "    ) & (\n",
    "        df[[col for col in df.columns if col.startswith('gpt_lemma')]].notna().sum(axis=1) == 1\n",
    "    )\n",
    "    \n",
    "    # Condizione 2: `gorman_lemma_1` e `gpt_lemma_1` sono uguali\n",
    "    first_lemmas_equal = df['gorman_lemma_1'] == df['gpt_lemma_1']\n",
    "\n",
    "    # Combina le condizioni\n",
    "    rows_to_remove = only_first_lemmas_non_empty & first_lemmas_equal\n",
    "\n",
    "    # Filtra il DataFrame per mantenere solo le righe che non soddisfano la condizione\n",
    "    filtered_df = df[~rows_to_remove]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aeschines-1-1-50-bu1.xml',\n",
       " 'aeschines-1-101-150-bu1.xml',\n",
       " 'aeschines-1-151-196-bu1.xml',\n",
       " 'aeschines-1-51-100-bu1.xml',\n",
       " 'antiphon-1-bu2.xml',\n",
       " 'antiphon-2-bu2.xml',\n",
       " 'antiphon-5-bu2.xml',\n",
       " 'antiphon-6-bu2.xml',\n",
       " 'appian-bc-1-0-1-4-bu1.xml',\n",
       " 'appian-bc-1-11-14-bu1.xml',\n",
       " 'appian-bc-1-5-7-bu1.xml',\n",
       " 'appian-bc-1-8-10-bu1.xml',\n",
       " 'aristotle-politics-book-1-bu1.xml',\n",
       " 'aristotle-politics-book-2-bu2.xml',\n",
       " 'athen12-1-9-2019.xml',\n",
       " 'athen12-10-19-2019.xml',\n",
       " 'athen12-20-29-2019.xml',\n",
       " 'athen12-30-39-jan-15.xml',\n",
       " 'athen12-40-49-jan-15.xml',\n",
       " 'athen12-50-59-jan-15.xml',\n",
       " 'athen12-60-69-jan-15.xml',\n",
       " 'athen12-70-81-jan-15.xml',\n",
       " 'athen13-1-9-jan-15.xml',\n",
       " 'athen13-10-19-jan-15.xml',\n",
       " 'athen13-20-29-jan-15.xml',\n",
       " 'athen13-30-39-jan-15.xml',\n",
       " 'athen13-40-49-jan-15.xml',\n",
       " 'athen13-50-59-jan-15.xml',\n",
       " 'athen13-60-69-jan-15.xml',\n",
       " 'athen13-70-79-jan-15.xml',\n",
       " 'athen13-80-89-jan-15.xml',\n",
       " 'athen13-90-95-jan-15.xml',\n",
       " 'dem-59-neaira-2019.xml',\n",
       " 'demosthenes-1-bu1.xml',\n",
       " 'demosthenes-18-1-50-bu2.xml',\n",
       " 'demosthenes-18-101-150-bu2.xml',\n",
       " 'demosthenes-18-151-200-bu2.xml',\n",
       " 'demosthenes-18-201-275-bu1.xml',\n",
       " 'demosthenes-18-276-324-bu1.xml',\n",
       " 'demosthenes-18-51-100-bu1.xml',\n",
       " 'demosthenes-4-phil1-bu1.xml',\n",
       " 'demosthenes-46-tree.xml',\n",
       " 'demosthenes-47-tree.xml',\n",
       " 'demosthenes-49-tree.xml',\n",
       " 'demosthenes-50-tree.xml',\n",
       " 'demosthenes-52-tree.xml',\n",
       " 'demosthenes-53-tree.xml',\n",
       " 'diodsic-11-1-20-bu4.xml',\n",
       " 'diodsic-11-81-92-bu1.xml',\n",
       " 'diodsic11-21-40-bu2.xml',\n",
       " 'diodsic11-41-60-bu1.xml',\n",
       " 'diodsic11-61-80-bu1.xml',\n",
       " 'dion-hal-1-1-15-bu2.xml',\n",
       " 'dion-hal-1-16-30-bu1.xml',\n",
       " 'dion-hal-1-31-45-bu1.xml',\n",
       " 'dion-hal-1-46-60-bu1.xml',\n",
       " 'dion-hal-1-61-75-bu1.xml',\n",
       " 'dion-hal-1-76-90-bu1.xml',\n",
       " 'hdt-1-1-19-bu3-2019.xml',\n",
       " 'hdt-1-100-119-bu3-2019.xml',\n",
       " 'hdt-1-120-149-bu2-2019.xml',\n",
       " 'hdt-1-150-169-bu3-2019.xml',\n",
       " 'hdt-1-170-189-bu2-2019.xml',\n",
       " 'hdt-1-190-216-bu2-2019.xml',\n",
       " 'hdt-1-20-39-bu2-2019.xml',\n",
       " 'hdt-1-40-59-bu2-2019.xml',\n",
       " 'hdt-1-60-79-bu2-2019.xml',\n",
       " 'hdt-1-80-99-bu5-2019.xml',\n",
       " 'josephus-bj-1-1-2-bu1.xml',\n",
       " 'josephus-bj-1-11-15-bu1.xml',\n",
       " 'josephus-bj-1-16-20-bu1.xml',\n",
       " 'josephus-bj-1-21-25-bu1.xml',\n",
       " 'josephus-bj-1-3-5-bu2.xml',\n",
       " 'josephus-bj-1-6-10-bu1.xml',\n",
       " 'lysias-1-bu1.xml',\n",
       " 'lysias-12-bu1.xml',\n",
       " 'lysias-13-bu1.xml',\n",
       " 'lysias-14-bu1.xml',\n",
       " 'lysias-15.xml',\n",
       " 'lysias-19-bu1.xml',\n",
       " 'lysias-23-bu1.xml',\n",
       " 'plato-apology.xml',\n",
       " 'plut-alcib-1-17-bu1.xml',\n",
       " 'plut-alcib-18-39-bu1.xml',\n",
       " 'plut-fortuna-romanorum-bu1.xml',\n",
       " 'plutarch-alex-fort-aut-virt-bu2.xml',\n",
       " 'plutarch-lycurgus-1-15-bu4.xml',\n",
       " 'plutarch-lycurgus-16-31-bu2.xml',\n",
       " 'polybius-10-1-10-bu1.xml',\n",
       " 'polybius-10-11-20-bu1.xml',\n",
       " 'polybius-10-21-35-bu2.xml',\n",
       " 'polybius-10-36-49-bu1.xml',\n",
       " 'polybius-2-1-10-bu1.xml',\n",
       " 'polybius-2-11-20-bu1.xml',\n",
       " 'polybius-2-21-30-bu2.xml',\n",
       " 'polybius-2-31-40-bu2.xml',\n",
       " 'polybius-2-41-50-bu1.xml',\n",
       " 'polybius-2-51-60-bu1.xml',\n",
       " 'polybius-2-61-71-bu2.xml',\n",
       " 'polybius-21-1-10-bu1.xml',\n",
       " 'polybius-21-11-20-bu1.xml',\n",
       " 'polybius-21-21-30-bu1.xml',\n",
       " 'polybius-21-31-47-bu1.xml',\n",
       " 'polybius-6-16-30-bu1.xml',\n",
       " 'polybius-6-2-15-bu1.xml',\n",
       " 'polybius-6-31-45-bu1.xml',\n",
       " 'polybius-6-46-58-bu1.xml',\n",
       " 'polybius-9-1-20-bu1.xml',\n",
       " 'polybius-9-21-33-bu1.xml',\n",
       " 'polybius-9-34-45-bu1.xml',\n",
       " 'polybius1-1-9-2017.xml',\n",
       " 'polybius1-10-19-2017.xml',\n",
       " 'polybius1-20-29-2017.xml',\n",
       " 'polybius1-30-39-2017.xml',\n",
       " 'polybius1-40-49-2017.xml',\n",
       " 'polybius1-50-59-2017.xml',\n",
       " 'polybius1-60-69-2017.xml',\n",
       " 'polybius1-70-79-2017.xml',\n",
       " 'polybius1-80-88-2017.xml',\n",
       " 'ps-xen-ath-pol-bu2.xml',\n",
       " 'thuc-1-1-20-bu5.xml',\n",
       " 'thuc-1-101-120-bu2.xml',\n",
       " 'thuc-1-121-146-bu3.xml',\n",
       " 'thuc-1-21-40-bu4.xml',\n",
       " 'thuc-1-41-60-bu3.xml',\n",
       " 'thuc-1-61-80-bu3.xml',\n",
       " 'thuc-1-81-100-bu2.xml',\n",
       " 'thuc-3-1-20-bu1.xml',\n",
       " 'thuc-3-21-40-bu1.xml',\n",
       " 'xen-cyr-1-1-2-bu1.xml',\n",
       " 'xen-cyr-1-3-4-bu1.xml',\n",
       " 'xen-cyr-1-5-bu1.xml',\n",
       " 'xen-cyr-1-6-bu1.xml',\n",
       " 'xen-cyr-7-1-3-tree.xml',\n",
       " 'xen-cyr-7-4-5-tree.xml',\n",
       " 'xen-cyr-8-1-8-4-bu1.xml',\n",
       " 'xen-cyr-8-5-7-bu1.xml',\n",
       " 'xen-cyr-8-8-bu1.xml',\n",
       " 'xen-hell-1-1-4-bu2.xml',\n",
       " 'xen-hell-1-5-7-bu1.xml',\n",
       " 'xen-hell-2-bu1.xml',\n",
       " 'xen-hell-3-bu1.xml']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir = os.listdir('output_gorman_no_anomalies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemma_dic_pedialion.pkl', 'rb') as file:         #sto importando il dizionario dei lemmi di pedalion\n",
    "    lemma_dic_pedialion = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per popolare le colonne per ogni riga del DataFrame -> data una riga, se trova una form corrispondente nel dizionario di pedalion mi popola i lemmi\n",
    "def popola_riga(row,dict_forms):\n",
    "    form = row['@form']\n",
    "    if form in dict_forms:\n",
    "        lemmas = dict_forms[form].get('lemmas', [])\n",
    "        #postags = dict_forms[form].get('postags', [])\n",
    "        # Assegniamo i valori dei lemmas nelle colonne corrispondenti\n",
    "        for idx, lemma in enumerate(lemmas):\n",
    "            row[f'lemma_pedalion_{idx+1}'] = lemma\n",
    "\n",
    "        # Assegniamo i valori dei postags nelle colonne corrispondenti\n",
    "        #for idx, postag in enumerate(postags):\n",
    "        #    row[f'postag_pedalion_{idx+1}'] = postag\n",
    "    return row\n",
    "\n",
    "def add_pedalion_lemmas(df, lemma_dict):      #prende in input dataframe unificati qui di gorman con gpt a cui si vuole concatenare pedalion. \n",
    "    #df = pd.concat(dataframes_unificati).reset_index(drop=True)[['@id','@form','@lemma','@postag','@relation']]      \n",
    "\n",
    "    # Dizionario di esempio\n",
    "    dict_forms = lemma_dict\n",
    "\n",
    "    # Determiniamo il numero massimo di elementi nelle liste \"lemmas\" e \"postags\"\n",
    "    max_lemmas = max(len(info.get('lemmas', [])) for info in dict_forms.values())           #vede quale form ha più lemmi e quella è la lunghezza massima per aggiungere tot lemma pedalion1, lemma pedalion 2 etc.\n",
    "    #max_postags = max(len(info.get('postags', [])) for info in dict_forms.values())\n",
    "\n",
    "    # Aggiungiamo le colonne per i lemmas           \n",
    "    for i in range(1, max_lemmas + 1):\n",
    "        df[f'lemma_pedalion_{i}'] = ''\n",
    "\n",
    "    # Aggiungiamo le colonne per i postags\n",
    "    #for i in range(1, max_postags + 1):\n",
    "    #    df[f'postag_pedalion_{i}'] = ''\n",
    "    df = df.apply(popola_riga, axis=1, args=(dict_forms,))                      #sto popolando le righe del dataframe per ogni form vado a cercare se la form esiste nel dizionario, se esiste prenso i lemmas e li metto in lemma pedalion 1, lemma pedalion 2...\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CICLO FOR PER FARE INSIEME QUELLO CHE SOPRA NEL CODICE FACEVO SINGOLARMENTE\n",
    "list_dir = os.listdir('output_gorman_no_anomalies')\n",
    "for i in range(9,9\n",
    "               ):                                   #per farli tutti metti come range(0,len(list_dir))\n",
    "    df = dataframes_unificati[i] #eschine 1-50\n",
    "    df = df[~df['@lemma'].str.contains('punc', na=False)] #tolto punteggiatura dai lemmi\n",
    "    df = df.reset_index(drop=True)\n",
    "    lemma_chat = pd.read_csv('lemmatizzazioni gorman con chatgpt/lemma_chat_'+list_dir[i][:-4]+'.csv',on_bad_lines='skip') #caricato i lemmi di chatgpt per Eschine 1-50\n",
    "    lemma_chat = lemma_chat .dropna(how='all') #tolto righe con nan da csv di gpt\n",
    "    lemma_chat = lemma_chat .reset_index(drop=True)\n",
    "\n",
    "\n",
    "    lemma_chat= remove_unwanted_rows( lemma_chat)\n",
    "\n",
    "    df1= df[['@form','@lemma','@postag', '@relation']]\n",
    "\n",
    "\n",
    "    merged_df = create_merged_dataframe_with_postags(df1, lemma_chat)\n",
    "\n",
    "    df_filtrato = remove_rows_with_equal_lemmas(merged_df)\n",
    "    df = add_pedalion_lemmas(df_filtrato,lemma_dic_pedialion)   #vuole i dataframe unificati del dataframe a cui voglio aggiungere quelli di pedalion. \n",
    "    df.to_csv('tabelle_confronto_lemmi_gpt_gorman/'+ list_dir[i][:-4] + 'senza_ripetizioni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOGLIE PUNTEGGIATURA DAI DATAFRAME \n",
    "#first_sent = list(df_prova[0]['@lemma'])\n",
    "#first_sent_all = df_prova[0].loc[:,['@form','@lemma']]\n",
    "import betacode.conv\n",
    "for lis in dataframes: \n",
    "   for sent in lis:\n",
    "      lemmas = []\n",
    "      #print(sent['@lemma'])\n",
    "      for word in list(sent['@lemma']):\n",
    "         if word == 'punc1':\n",
    "            lemmas.append(word)\n",
    "         else:\n",
    "            lemmas.append(betacode.conv.beta_to_uni(betacode.conv.uni_to_beta(word)))\n",
    "      sent['@lemma'] = lemmas      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIZIONARIO GORMAN-GPT-PEDALION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_dir = os.listdir('correzione lemmi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_dir = os.listdir('correzione lemmi')\n",
    "df_lis = [pd.read_csv('correzione lemmi/'+i) for i in lis_dir]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic(df):\n",
    "    # Define the columns (adjust as needed)\n",
    "    lemma_cols = [\n",
    "            'gorman_lemma_1',\n",
    "    'gorman_lemma_2',\n",
    "    'gorman_lemma_3',\n",
    "    'gorman_lemma_4',\n",
    "    'gorman_lemma_5',\n",
    "    'gorman_lemma_6',\n",
    "    'gorman_lemma_7',\n",
    "    'gorman_lemma_8',\n",
    "    'gorman_lemma_9',\n",
    "    'gorman_lemma_10',\n",
    "    'gorman_lemma_11',\n",
    "    'gorman_lemma_12',\n",
    "    'gorman_lemma_13',\n",
    "    'gorman_lemma_14',\n",
    "    'gpt_lemma_1',\n",
    "    'gpt_lemma_2',\n",
    "    'gpt_lemma_3',\n",
    "    'gpt_lemma_4',\n",
    "    'gpt_lemma_5',\n",
    "    'gpt_lemma_6',\n",
    "    'gpt_lemma_7',\n",
    "    'gpt_lemma_8',\n",
    "    'gpt_lemma_9',\n",
    "    'gpt_lemma_10',\n",
    "    'gpt_lemma_11',\n",
    "    'gpt_lemma_12',\n",
    "    'gpt_lemma_13',\n",
    "    'gpt_lemma_14',\n",
    "            'lemma_pedalion_1', 'lemma_pedalion_2', 'lemma_pedalion_3',\n",
    "            'lemma_pedalion_4', 'lemma_pedalion_5', 'lemma_pedalion_6',\n",
    "            'lemma_pedalion_7', 'lemma_pedalion_8', 'lemma_pedalion_9',\n",
    "            'lemma_pedalion_10', 'lemma_pedalion_11', 'lemma_pedalion_12',\n",
    "            'lemma_pedalion_13', 'lemma_pedalion_14', 'lemma_pedalion_15',\n",
    "            'lemma_pedalion_16', 'lemma_pedalion_17'\n",
    "        ]\n",
    "    postag_cols = ['postag_1',\n",
    "    'postag_2',\n",
    "    'postag_3',\n",
    "    'postag_4',\n",
    "    'postag_5',\n",
    "    'postag_6',\n",
    "    'postag_7',\n",
    "    'postag_8',\n",
    "    'postag_9',\n",
    "    'postag_10',\n",
    "    'postag_11',\n",
    "    'postag_12',\n",
    "    'postag_13',\n",
    "    'postag_14',\n",
    "    'postag_15',\n",
    "    'postag_16',\n",
    "    'postag_17',\n",
    "    'postag_18',\n",
    "    'postag_19']\n",
    "\n",
    "    # Dictionary to store consolidated data per @form\n",
    "    form_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        form = row['@form']\n",
    "        if form not in form_dict:\n",
    "            form_dict[form] = {\"lemmas\": set(), \"postags\": set()}\n",
    "        # Gather lemma values (avoiding missing values)\n",
    "        for col in lemma_cols:\n",
    "            if col in df.columns:\n",
    "                value = row[col]\n",
    "                if pd.notnull(value):\n",
    "                    form_dict[form][\"lemmas\"].add(value)\n",
    "        # Gather postag values similarly\n",
    "        for col in postag_cols:\n",
    "            if col in df.columns:\n",
    "                value = row[col]\n",
    "                if pd.notnull(value):\n",
    "                    form_dict[form][\"postags\"].add(value)\n",
    "    return form_dict\n",
    "\n",
    "def merge_list_of_dicts(dict_list):\n",
    "    merged = {}\n",
    "    for d in dict_list:\n",
    "        for form, values in d.items():\n",
    "            if form in merged:\n",
    "                merged[form]['lemmas'].update(values['lemmas'])\n",
    "                merged[form]['postags'].update(values['postags'])\n",
    "            else:\n",
    "                merged[form] = {\n",
    "                    'lemmas': set(values['lemmas']),\n",
    "                    'postags': set(values['postags'])\n",
    "                }\n",
    "    return merged\n",
    "\n",
    "lis_dir = os.listdir('correzione lemmi')\n",
    "lis = [pd.read_csv('correzione lemmi/'+i) for i in lis_dir]\n",
    "      \n",
    "dic_lis = []\n",
    "for df in lis:\n",
    "    dic = create_dic(df)\n",
    "    dic_lis.append(dic)\n",
    "merged_dic = merge_list_of_dicts(dic_lis)  \n",
    "def create_dic_exclusion(df,dict):\n",
    "    # Define the columns (adjust as needed)\n",
    "    lemma_cols = [\n",
    "        'gorman_lemma_1',\n",
    " 'gorman_lemma_2',\n",
    " 'gorman_lemma_3',\n",
    " 'gorman_lemma_4',\n",
    " 'gorman_lemma_5',\n",
    " 'gorman_lemma_6',\n",
    " 'gorman_lemma_7',\n",
    " 'gorman_lemma_8',\n",
    " 'gorman_lemma_9',\n",
    " 'gorman_lemma_10',\n",
    " 'gorman_lemma_11',\n",
    " 'gorman_lemma_12',\n",
    " 'gorman_lemma_13',\n",
    " 'gorman_lemma_14',\n",
    "'gpt_lemma_1',\n",
    " 'gpt_lemma_2',\n",
    " 'gpt_lemma_3',\n",
    " 'gpt_lemma_4',\n",
    " 'gpt_lemma_5',\n",
    " 'gpt_lemma_6',\n",
    " 'gpt_lemma_7',\n",
    " 'gpt_lemma_8',\n",
    " 'gpt_lemma_9',\n",
    " 'gpt_lemma_10',\n",
    " 'gpt_lemma_11',\n",
    " 'gpt_lemma_12',\n",
    " 'gpt_lemma_13',\n",
    " 'gpt_lemma_14',\n",
    "        'lemma_pedalion_1', 'lemma_pedalion_2', 'lemma_pedalion_3',\n",
    "        'lemma_pedalion_4', 'lemma_pedalion_5', 'lemma_pedalion_6',\n",
    "        'lemma_pedalion_7', 'lemma_pedalion_8', 'lemma_pedalion_9',\n",
    "        'lemma_pedalion_10', 'lemma_pedalion_11', 'lemma_pedalion_12',\n",
    "        'lemma_pedalion_13', 'lemma_pedalion_14', 'lemma_pedalion_15',\n",
    "        'lemma_pedalion_16', 'lemma_pedalion_17'\n",
    "    ]\n",
    "    postag_cols = ['postag_1',\n",
    " 'postag_2',\n",
    " 'postag_3',\n",
    " 'postag_4',\n",
    " 'postag_5',\n",
    " 'postag_6',\n",
    " 'postag_7',\n",
    " 'postag_8',\n",
    " 'postag_9',\n",
    " 'postag_10',\n",
    " 'postag_11',\n",
    " 'postag_12',\n",
    " 'postag_13',\n",
    " 'postag_14',\n",
    " 'postag_15',\n",
    " 'postag_16',\n",
    " 'postag_17',\n",
    " 'postag_18',\n",
    " 'postag_19']\n",
    "\n",
    "    # Dictionary to store consolidated data per @form\n",
    "    form_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        form = row['@form']\n",
    "        if form not in dict:\n",
    "            if form not in form_dict:\n",
    "                form_dict[form] = {\"lemmas\": set(), \"postags\": set()}\n",
    "            # Gather lemma values (avoiding missing values)\n",
    "            for col in lemma_cols:\n",
    "                if col in df.columns:\n",
    "                    value = row[col]\n",
    "                    if pd.notnull(value):\n",
    "                        form_dict[form][\"lemmas\"].add(value)\n",
    "            # Gather postag values similarly\n",
    "            for col in postag_cols:\n",
    "                if col in df.columns:\n",
    "                    value = row[col]\n",
    "                    if pd.notnull(value):\n",
    "                        form_dict[form][\"postags\"].add(value)\n",
    "    return form_dict\n",
    "lis_dir = os.listdir('tabelle_confronto_lemmi_gpt_gormanok')\n",
    "lis = [pd.read_csv('tabelle_confronto_lemmi_gpt_gormanok/'+i) for i in lis_dir]\n",
    "      \n",
    "\n",
    "dic_lis = []\n",
    "for df in lis:\n",
    "    dic = create_dic_exclusion(df,merged_dic)\n",
    "    dic_lis.append(dic)\n",
    "#aggiungo alla lista il dizionazio pulito\n",
    "dic_lis.append(merged_dic)  \n",
    "#creao un unico dizionario pulito finale\n",
    "final_dict = merge_list_of_dicts(dic_lis)\n",
    "#Visualizzazione\n",
    "# Optionally, convert sets to sorted lists for easier viewing\n",
    "for form, values in final_dict.items():\n",
    "    values['lemmas'] = sorted(list(values['lemmas']))\n",
    "    values['postags'] = sorted(list(values['postags']))\n",
    "\n",
    "# Now, merged_dic is a unique dictionary with no repetitions.\n",
    "# To convert this merged dictionary to a DataFrame:\n",
    "df_to_see = pd.DataFrame([\n",
    "    {'@form': form, 'lemmas': values['lemmas'], 'postags': values['postags']}\n",
    "    for form, values in final_dict.items()\n",
    "])\n",
    "\n",
    "\n",
    "lemmas_expanded = pd.DataFrame(df_to_see ['lemmas'].tolist(), index=df_to_see.index)\n",
    "lemmas_expanded.columns = [f'lemma_{i+1}' for i in range(lemmas_expanded.shape[1])]\n",
    "\n",
    "# For the 'postags' column\n",
    "postags_expanded = pd.DataFrame(df_to_see ['postags'].tolist(), index=df_to_see.index)\n",
    "postags_expanded.columns = [f'postag_{i+1}' for i in range(postags_expanded.shape[1])]\n",
    "\n",
    "# Step 3: Concatenate the @form column with the new expanded columns\n",
    "final_df = pd.concat([df_to_see [['@form']], lemmas_expanded, postags_expanded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@form</th>\n",
       "      <th>lemma_1</th>\n",
       "      <th>lemma_2</th>\n",
       "      <th>lemma_3</th>\n",
       "      <th>lemma_4</th>\n",
       "      <th>lemma_5</th>\n",
       "      <th>lemma_6</th>\n",
       "      <th>lemma_7</th>\n",
       "      <th>lemma_8</th>\n",
       "      <th>lemma_9</th>\n",
       "      <th>...</th>\n",
       "      <th>lemma_19</th>\n",
       "      <th>lemma_20</th>\n",
       "      <th>postag_1</th>\n",
       "      <th>postag_2</th>\n",
       "      <th>postag_3</th>\n",
       "      <th>postag_4</th>\n",
       "      <th>postag_5</th>\n",
       "      <th>postag_6</th>\n",
       "      <th>postag_7</th>\n",
       "      <th>postag_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ἀνθρωπος</td>\n",
       "      <td>̓́Ανθρωπος</td>\n",
       "      <td>ἄνθρωπος</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>n-s---mn-</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ἀλέξιδος</td>\n",
       "      <td>Ἀλέξιδος</td>\n",
       "      <td>Ἀλέξις</td>\n",
       "      <td>Ἄλεξις</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>n-s---mg-</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Τιμόκρατες</td>\n",
       "      <td>Τιμοκράτης</td>\n",
       "      <td>Τιμόκρατες</td>\n",
       "      <td>Τιμόκρατης</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>n-s---mv-</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>τἀπιτήδει᾽</td>\n",
       "      <td>τὰ ἐπιτήδεια</td>\n",
       "      <td>ἐπιτήδειος</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a-p---na-</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ἦν</td>\n",
       "      <td></td>\n",
       "      <td>eἰμί</td>\n",
       "      <td>είμί</td>\n",
       "      <td>εἰμί</td>\n",
       "      <td>εἰμί (3s)</td>\n",
       "      <td>εἰμί (3sg.impf.)</td>\n",
       "      <td>εἰμί (impf. 3s)</td>\n",
       "      <td>εἰμί (impf.)</td>\n",
       "      <td>εἰμί (impf.3s)</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>v1siia---</td>\n",
       "      <td>v3siia---</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26042</th>\n",
       "      <td>εἰσηγεῖσθαι</td>\n",
       "      <td>εἰσηγέομαι</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26043</th>\n",
       "      <td>εἴ-τ᾽</td>\n",
       "      <td>εἴτε</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26044</th>\n",
       "      <td>ῥᾴδίως</td>\n",
       "      <td>ῥᾴδιος</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26045</th>\n",
       "      <td>ὧδὶ</td>\n",
       "      <td>ὧδὶ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26046</th>\n",
       "      <td>ξενικός</td>\n",
       "      <td>ξενικός</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26047 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             @form       lemma_1     lemma_2     lemma_3 lemma_4    lemma_5  \\\n",
       "0         Ἀνθρωπος    ̓́Ανθρωπος    ἄνθρωπος        None    None       None   \n",
       "1         Ἀλέξιδος      Ἀλέξιδος      Ἀλέξις      Ἄλεξις    None       None   \n",
       "2       Τιμόκρατες    Τιμοκράτης  Τιμόκρατες  Τιμόκρατης    None       None   \n",
       "3       τἀπιτήδει᾽  τὰ ἐπιτήδεια  ἐπιτήδειος        None    None       None   \n",
       "4               ἦν                      eἰμί        είμί    εἰμί  εἰμί (3s)   \n",
       "...            ...           ...         ...         ...     ...        ...   \n",
       "26042  εἰσηγεῖσθαι    εἰσηγέομαι        None        None    None       None   \n",
       "26043        εἴ-τ᾽          εἴτε        None        None    None       None   \n",
       "26044       ῥᾴδίως        ῥᾴδιος        None        None    None       None   \n",
       "26045          ὧδὶ           ὧδὶ        None        None    None       None   \n",
       "26046      ξενικός       ξενικός        None        None    None       None   \n",
       "\n",
       "                lemma_6          lemma_7       lemma_8         lemma_9  ...  \\\n",
       "0                  None             None          None            None  ...   \n",
       "1                  None             None          None            None  ...   \n",
       "2                  None             None          None            None  ...   \n",
       "3                  None             None          None            None  ...   \n",
       "4      εἰμί (3sg.impf.)  εἰμί (impf. 3s)  εἰμί (impf.)  εἰμί (impf.3s)  ...   \n",
       "...                 ...              ...           ...             ...  ...   \n",
       "26042              None             None          None            None  ...   \n",
       "26043              None             None          None            None  ...   \n",
       "26044              None             None          None            None  ...   \n",
       "26045              None             None          None            None  ...   \n",
       "26046              None             None          None            None  ...   \n",
       "\n",
       "      lemma_19 lemma_20   postag_1   postag_2 postag_3 postag_4 postag_5  \\\n",
       "0         None     None  n-s---mn-       None     None     None     None   \n",
       "1         None     None  n-s---mg-       None     None     None     None   \n",
       "2         None     None  n-s---mv-       None     None     None     None   \n",
       "3         None     None  a-p---na-       None     None     None     None   \n",
       "4         None     None  v1siia---  v3siia---     None     None     None   \n",
       "...        ...      ...        ...        ...      ...      ...      ...   \n",
       "26042     None     None       None       None     None     None     None   \n",
       "26043     None     None       None       None     None     None     None   \n",
       "26044     None     None       None       None     None     None     None   \n",
       "26045     None     None       None       None     None     None     None   \n",
       "26046     None     None       None       None     None     None     None   \n",
       "\n",
       "      postag_6 postag_7 postag_8  \n",
       "0         None     None     None  \n",
       "1         None     None     None  \n",
       "2         None     None     None  \n",
       "3         None     None     None  \n",
       "4         None     None     None  \n",
       "...        ...      ...      ...  \n",
       "26042     None     None     None  \n",
       "26043     None     None     None  \n",
       "26044     None     None     None  \n",
       "26045     None     None     None  \n",
       "26046     None     None     None  \n",
       "\n",
       "[26047 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('dizionario_gpt_gorman_pedalion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TESI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
